## Training Evaluator LMs

Lastly, some might be interested in training their own evaluator language models for various purposes. Our training code is based on `alignment-handbook` respoitory and located in the `train` directory. Please refer to [Installation Instructions](train/README.md/#installation-instructions) for environment setup and [Instructions to Replicate Prometheus-v2.0](train/recipes/prometheus-v2.0/README.md) for training commands.

### Train Data Preparation

In order to (1) replicate Prometheus 2, (2) train a new evaluator LM for scratch or (3) continually train Prometheus on your custom feedback data, you should prepare a JSON file that contains a list of dictionaries. Each dictionary should be consisted as follows:

#### Direct Assessment Training Data Format:

<details open>
  <summary><code>>>> Data Format to Train Evaluator LMs on Direct Assessment (JSON)</code></summary>

```json 
{
  "orig_instruction": "I'm a software developer who has been working on a project to create a new AI model. This model is supposed to interpret ambiguous queries or statements based on the context and deliver appropriate replies. However, I'm running into a problem. Despite the fact that I have trained the model using several datasets and used various techniques, the model is still struggling to correctly interpret ambiguous queries. Sometimes, it even gives irrelevant or illogical responses. Can you suggest any strategies or approaches to improve the ability of the model to decipher ambiguity accurately and consistently?",
  "orig_response": "AI models are sometimes not perfect, and it's just natural for them to have issues with ambiguity. It's great that you've trained your model, but maybe it's just the nature of AI to not be able to handle ambiguous queries well. Have you considered that maybe the AI doesn't need to be perfect? You can still use the AI and accept that it might not always understand the queries correctly. People can always rephrase their queries if the AI doesn't understand them the first time. Remember, AI is still a developing field, and it's okay for it to not be perfect all the time.",
  "orig_reference_answer": "Indeed, interpreting ambiguous queries is a common challenge in the field of AI and machine learning. Here are a few strategies you could adopt to improve the performance of your model in this regards: 1. **Enhance Training Data**: The quality and diversity of training data plays a crucial role in how well an AI model performs. Try to include as many ambiguous queries as possible in your training data. Also, make sure that the data covers a diverse range of topics and contexts. 2. **Contextual Understanding**: Incorporate mechanisms in your model that allow it to consider the context when interpreting queries. One way to do this is by using Recurrent Neural Networks (RNN), particularly Long-Short Term Memory (LSTM) layers, which are designed to remember past information and use it in the interpretation of current data. 3. **Transfer Learning**: This involves leveraging the knowledge gained from training one model on a particular task and applying it to a different but related task. Models like BERT, GPT, and ELMo are pre-trained on a large corpus of data and have been found to be quite effective at understanding context and ambiguity. 4. **Attention Mechanisms**: These guide the model to focus on specific parts of the input when producing an output. This can be particularly useful for interpreting ambiguous queries, as the model can learn to focus on the parts of the input that resolve the ambiguity. 5. **Regular Evaluation and Fine-tuning**: Regularly evaluate the performance of your model using appropriate metrics and fine-tune it based on the results. Tools like Confusion Matrix, Precision-Recall, ROC curves, etc can help in understanding the areas where your model is lacking. Remember, building an AI model that can consistently decode ambiguity accurately is a complex task that might require a considerable amount of time and effort. Be patient and persistent, and with enough fine-tuning, your model's performance should improve over time.",
  "orig_criteria":"What is the ability of the model to correctly interpret ambiguous queries or statements based on the context and deliver appropriate replies?",
  "orig_score1_description":"The model is unable to comprehend ambiguous inquiries, resulting in irrelevant or illogical responses.",
  "orig_score2_description":"The model can occasionally decipher ambiguity correctly, however, its responses frequently miss the mark.",
  "orig_score3_description":"The model frequently decodes ambiguity correctly, but may occasionally offer irrelevant replies.",
  "orig_score4_description":"The model generally deciphers ambiguity accurately, delivering suitable responses the majority of the time.",
  "orig_score5_description":"The model consistently decodes ambiguity accurately, offering highly pertinent responses according to the context.",
  "orig_feedback":"The response fails to provide any meaningful strategies or approaches to improve the model's ability to decipher ambiguity accurately and consistently. It does not address the main concern mentioned in the instruction, which is to suggest ways to enhance the model's performance. Instead, the response suggests accepting the AI's imperfections and encouraging users to rephrase their queries. This does not align with the goal of developing an AI model capable of interpreting ambiguous queries or statements based on context. The response is also much shorter than the required 18 sentences. So the overall score is 1.",
  "orig_score":"1"
}
```

Each dictionary consists of 11 keys: "orig_instruction", "orig_response", "orig_reference_answer", "orig_criteria", "orig_score1_description", "orig_score2_description", "orig_score3_description", "orig_score4_description", "orig_score5_description", "orig_feedback", and "orig_score". <br>
<br>

\* To train the Prometheus 2 models, we used the [Feedback Collection](https://huggingface.co/datasets/prometheus-eval/Feedback-Collection).
</details>

#### Pairwise Ranking Training Data Format:

<details open>
  <summary><code>>>> Data Format to Train Evaluator LMs on Pairwise Ranking (JSON)</code></summary>

```json 
{
  "orig_instruction": "A group of historians are conducting a debate on the factors that led to the fall of the Roman Empire. One historian argues that the primary reason for the fall was the constant pressure from barbarian invasions. Another one believes it was because of economic troubles and overreliance on slave labor. A third one suggests it was due to moral decay and political instability. Each historian needs to provide evidence to support their claims. How would the historian arguing for economic troubles and overreliance on slave labor present their case?",
  "orig_response_A": "The argument that economic troubles and overreliance on slave labor caused the fall of the Roman Empire is substantial. Firstly, there was the issue of the devaluation of the Roman currency, which according to Harper (2016) led to rampant inflation destabilizing the economy. Secondly, the empire's reliance on slave labor is another crucial factor. Scheidel (2007) highlights how this led to unemployment among free Roman citizens, destabilizing the economy further. Lastly, the empire's heavy reliance on imports like grain from Egypt also created economic problems. As per Temin (2006), this led to a trade deficit, weakening the Roman economy when it began to lose territories. In conclusion, the combination of economic instability, overreliance on slave labor, and a destabilizing trade deficit substantially contributed to the fall of the Roman Empire.",
  "orig_response_B": "The fall of the Roman Empire can be attributed to economic troubles and an overreliance on slave labor. One of the reasons for this was the decrease in value of Roman currency. The economy was unstable because of this. In addition, the reliance on slaves for work caused job scarcity among free citizens. Slaves were cheap to maintain, which led to a decrease in job opportunities, and this in turn led to economic instability. The Roman Empire's expansion towards the east also caused them to rely heavily on imported goods like grain from Egypt. This led to a trade imbalance which further weakened the economy. The loss of territories made it hard to maintain the imbalance, causing more economic decline. Thus, it can be said that the fall of the Roman Empire was largely due to economic issues and the dependence on slave labor.",
  "orig_reference_answer": "This argument focuses on the economic troubles and overreliance on slave labor as primary reasons for the fall of the Roman Empire. To start with, one of the significant pieces of evidence is the devaluation of Roman currency. As highlighted by Harper (2016), the empire suffered from severe inflation due to the constant debasement of their currency, making it difficult for the economy to remain stable. Moreover, the overreliance on slave labor also played a detrimental role. As pointed out by Scheidel (2007), the dependence on slaves led to unemployment among free Roman citizens. This is because slaves were significantly cheaper to maintain compared to hiring free citizens, leading to a decline in job opportunities, which in turn resulted in economic instability. Furthermore, the empire's expansion to the east made them highly dependent on imports, for instance, grain from Egypt. As noted by Temin (2006), this created a trade deficit that further weakened the Roman economy. When the empire began to lose its territories, it became increasingly difficult to maintain this trade imbalance, leading to economic decline. In conclusion, it can be argued that the economic troubles, mainly due to the devaluation of currency and overreliance on slave labor, were significant contributing factors to the fall of the Roman Empire. The evidence provided, which includes scholarly references to Harper (2016), Scheidel (2007), and Temin (2006), supports this thesis.",
  "orig_criteria": "Is the answer well supported with evidence, including citations/attributions wherever relevant?",
  "orig_feedback": "pon examining the evidence presented in both responses, there are several factors to consider in relation to the provided score rubric which emphasizes the importance of well-supported arguments with relevant citations. Response A cites Harper (2016) on the devaluation of Roman currency and its contribution to inflation and economic instability, aligning well with the criteria of providing evidence. It also references Scheidel (2007) to demonstrate how the overreliance on slave labor led to unemployment among free citizens, which is a critical aspect of the economic argument. Furthermore, Temin (2006) is used to illustrate the issues with trade deficits due to reliance on imported goods like grain, providing a well-rounded explanation backed by scholarly sources. Response B, while covering similar points, does not provide specific citations and thus lacks the same level of evidential support seen in Response A. The arguments in Response B are more generalized statements about currency devaluation, slave labor reliance, and trade imbalances, but without the authoritative backing of scholarly references. While it mentions the economic decline and the dependence on slave labor, the absence of cited evidence means it does not meet the score rubric criteria as effectively as Response A. Both responses draw upon the economic factors and reliance on slave labor as central to their arguments, but Response A's inclusion of specific references and the contextualization of those references with respect to their impact on the Roman Empire make it a more robust and credible argument according to the evaluation criteria. Therefore, considering the importance of well-supported evidence, including citations, as stipulated by the score rubric, Response A is determined to be better supported than Response B.",
  "orig_preference": "A"
}
```
Each dictionary consists of 7 keys: "orig_instruction", "orig_response_A", "orig_response_B", "orig_reference_answer", "orig_criteria", "orig_feedback", and "orig_preference". <br>
<br>

\* To train the Prometheus 2 models, we used the [Preference Collection](https://huggingface.co/datasets/prometheus-eval/Preference-Collection).
</details>

<br>

### Evaluation of Evaluator LMs

*Note*: Evaluation of evaluator LMs will be available in the next release. Stay tuned!

Prometheus-Eval supports evaluating the performance of evaluator language models (LLMs) using direct assessment and pairwise ranking evaluation formats. Supported benchmarks include VicunaBench, MT-Bench, and FLASK for direct assessment, and HHH Alignment, MT Bench Human Judgment, and Auto-J Eval for pairwise ranking. The evaluation results are reported in terms of Pearson correlation and agreement with human judgments.

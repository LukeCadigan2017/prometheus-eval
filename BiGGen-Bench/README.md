# üåü BiGGen-Bench üöÄ

BiGGen-Bench is a dedicated benchmarking platform designed to evaluate the nuanced capabilities of Large Language Models across a variety of complex and diverse tasks.

## Features

- **Evaluation Scope**: Covers nine key capabilities of LLMs across 77 tasks, with 765 unique instances tailored to test specific aspects of model performance.
- **Scoring System**: Utilizes a detailed scoring rubric from 1 to 5, reflecting a range of outcomes based on instance-specific criteria closely aligned with the nuanced requirements of each task.
- **Transparency and Openness**: All codes, data, and detailed evaluation results are publicly available to foster transparency and enable community-driven enhancements and verifications.


## üìã Prerequisites

Before you dive in, make sure you have the following:

- **Python 3.10+**: The scripts are tested with Python 3.7 and later versions. You can download Python from [here](https://www.python.org/downloads/).
- **Pip**: Python's package installer. It usually comes with Python; make sure it's updated to the latest version using `python -m pip install --upgrade pip`.
- **Virtual Environment** (optional but recommended)

## üöÄ Installation

First, clone the repository and move to the project directory.

```bash
git clone https://github.com/prometheus-eval/prometheus-eval.git
cd prometheus-eval
cd BiGGen-Bench
```

Install the necessary Python packages:

```bash
pip install -r requirements.txt
```

This will install all required libraries, including `promethues-eval`, `vllm`, `huggingface_hub`, `pandas`, `transformers`, and others that are crucial for running the scripts.

## üìÅ Project Structure

The toolkit contains several scripts categorized based on their functionality:

- **Inference Scripts**:
  - `run_api_inference.py`: Runs inference using AsyncLiteLLM from a lite version of language models.
  - `run_base_inference.py`: Executes inference with base pre-trained models and handles specific formatting.
  - `run_chat_inference.py`: Specializes in generating responses for chat-based interactions using AutoTokenizer.
  
- **Evaluation Scripts**:
  - `run_response_eval.py`: Evaluates the responses generated by inference scripts using various evaluation metrics.
  - `make_table.py`: Generates a summary table from the evaluation results, presenting average scores and insights.

Each script is equipped with command-line interface (CLI) support for easy configuration and execution.

## üñ•Ô∏è Usage

Here's how to run the scripts:

1. **Running Inference**:
   - For API model inference:
     ```bash
     python run_api_inference.py --model_name "your-model-name" --output_file_path "./outputs/api_response.json"
     ```

   - For base model inference:
     ```bash
     python run_base_inference.py --model_name "your-model-name" --output_file_path "./outputs/base_response.json"
     ```

   - For chat model inference:
     ```bash
     python run_chat_inference.py --model_name "your-model-name" --output_file_path "./outputs/chat_response.json"
     ```


2. **Evaluating Responses**:
   - To evaluate responses and generate feedback:
     ```bash
     python run_response_eval.py --model_name "prometheus-eval/prometheus-7b-v2.0" --input_file_path "./outputs/api_response.json" --output_file_path "./feedback/evaluated.json"
     ```

3. **Generating Reports**:
   - To create a performance report from the evaluated feedback:
     ```bash
     python make_table.py --feedback_file_path "./feedback/evaluated.json"
     ```
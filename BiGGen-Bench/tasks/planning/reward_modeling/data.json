[
    {
        "capability": "planning",
        "task": "reward_modeling",
        "instance_idx": 0,
        "system_prompt": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs. As an example,\nthe reward function signature can be: @torch.jit.script\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    ...\n    return reward, {}\n\nSince the reward function will be decorated with @torch.jit.script,\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward,\n    (2) a dictionary of each individual reward component.\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
        "input": "### User: The Python environment is class Humanoid(VecTask):\\n    \"\"\"Rest of the environment definition omitted.\"\"\"\\n    def compute_observations(self):\\n        self.gym.refresh_dof_state_tensor(self.sim)\\n        self.gym.refresh_actor_root_state_tensor(self.sim)\\n\\n        self.gym.refresh_force_sensor_tensor(self.sim)\\n        self.gym.refresh_dof_force_tensor(self.sim)\\n        self.obs_buf[:], self.potentials[:], self.prev_potentials[:], self.up_vec[:], self.heading_vec[:] = compute_humanoid_observations(\\n            self.obs_buf, self.root_states, self.targets, self.potentials,\\n            self.inv_start_rot, self.dof_pos, self.dof_vel, self.dof_force_tensor,\\n            self.dof_limits_lower, self.dof_limits_upper, self.dof_vel_scale,\\n            self.vec_sensor_tensor, self.actions, self.dt, self.contact_force_scale, self.angular_velocity_scale,\\n            self.basis_vec0, self.basis_vec1)\\n\\ndef compute_humanoid_observations(obs_buf, root_states, targets, potentials, inv_start_rot, dof_pos, dof_vel,\\n                                  dof_force, dof_limits_lower, dof_limits_upper, dof_vel_scale,\\n                                  sensor_force_torques, actions, dt, contact_force_scale, angular_velocity_scale,\\n                                  basis_vec0, basis_vec1):\\n    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Tensor, Tensor, float, float, float, Tensor, Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]\\n\\n    torso_position = root_states[:, 0:3]\\n    torso_rotation = root_states[:, 3:7]\\n    velocity = root_states[:, 7:10]\\n    ang_velocity = root_states[:, 10:13]\\n\\n    to_target = targets - torso_position\\n    to_target[:, 2] = 0\\n\\n    prev_potentials_new = potentials.clone()\\n    potentials = -torch.norm(to_target, p=2, dim=-1) / dt\\n\\n    torso_quat, up_proj, heading_proj, up_vec, heading_vec = compute_heading_and_up(\\n        torso_rotation, inv_start_rot, to_target, basis_vec0, basis_vec1, 2)\\n\\n    vel_loc, angvel_loc, roll, pitch, yaw, angle_to_target = compute_rot(\\n        torso_quat, velocity, ang_velocity, targets, torso_position)\\n\\n    roll = normalize_angle(roll).unsqueeze(-1)\\n    yaw = normalize_angle(yaw).unsqueeze(-1)\\n    angle_to_target = normalize_angle(angle_to_target).unsqueeze(-1)\\n    dof_pos_scaled = unscale(dof_pos, dof_limits_lower, dof_limits_upper)\\n\\n    obs = torch.cat((torso_position[:, 2].view(-1, 1), vel_loc, angvel_loc * angular_velocity_scale,\\n                     yaw, roll, angle_to_target, up_proj.unsqueeze(-1), heading_proj.unsqueeze(-1),\\n                     dof_pos_scaled, dof_vel * dof_vel_scale, dof_force * contact_force_scale,\\n                     sensor_force_torques.view(-1, 12) * contact_force_scale, actions), dim=-1)\\n\\n    return obs, potentials, prev_potentials_new, up_vec, heading_vec\\n\\n\\n\\n. Write a reward function for the following task: to make the humanoid run as fast as possible.\n\n### Assistant: ```python\nimport torch\nfrom torch import Tensor\nfrom typing import Dict, Tuple\n\n\n@torch.jit.script\ndef compute_reward(root_states: Tensor, velocity: Tensor, dt: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n\n    # extract the torso_position\n    torso_position = root_states[:, 0:3]\n    \n    # calculate the torso velocity by differentiating the position with respect to time. \n    torso_velocity = (torch.norm(velocity, dim=-1)).view(-1, 1)\n    \n    # Scale the torso_velocity to encourage the humanoid to run as fast as possible. \n    speed_reward: Tensor = torso_velocity / dt\n    speed_reward_temp: float = 0.1\n    scaled_speed_reward: Tensor = torch.exp(speed_reward / speed_reward_temp)\n    \n    # We want to encourage the humanoid to keep its footing, so we reward if the torso is above a certain position. \n    # extract the z-axis position of the torso\n    torso_height = torso_position[:, 2].view(-1, 1)\n    torso_height_temp: float = 0.1\n\n    # The height of the humanoid's torso when it is standing up is approximately 1. The height when it is fallen down is approximately 0.2.\n    # We give a reward if the height is above 0.2 to encourage the humanoid to stand up and run.\n    height_reward = torch.where(torso_height > 0.2, torch.ones_like(torso_height), torch.zeros_like(torso_height))\n    scaled_height_reward: Tensor = torch.exp(height_reward / torso_height_temp)\n\n    reward = scaled_speed_reward + scaled_height_reward\n    reward_components: Dict[str, Tensor] = {\"scaled_speed_reward\": scaled_speed_reward, \"scaled_height_reward\": scaled_height_reward}\n\n    return reward, reward_components\n```\nThe `compute_reward` function calculates a reward based on two factors:\n\n1. `scaled_speed_reward`: This is a reward that encourages the humanoid to run as fast as possible. The speed is calculated as the norm of the velocity and is scaled by the scalar `speed_reward_temp`.\n\n2. `scaled_height_reward`: This is a reward that encourages the humanoid to keep its footing and not fall down. The humanoid gets a reward if the height of its torso is above a certain threshold. The height reward is scaled by the scalar `torso_height_temp`.\n\nThese two rewards are then summed to get the total reward.\n\n### User: We trained a RL policy using the provided reward function code and it seems that the humaniod is frequently falling down.\n\n### Assistant: ",
        "reference_answer": "```python\nimport torch\nfrom torch import Tensor\nfrom typing import Dict, Tuple\n\n\n@torch.jit.script\ndef compute_reward(root_states: Tensor, velocity: Tensor, dof_force: Tensor, dt: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n\n    # Extract the torso position\n    torso_position = root_states[:, 0:3]\n\n    # Calculate the speed by differentiating the position with respect to time\n    velocity_norm: Tensor = (torch.norm(velocity, dim=-1)).view(-1, 1)\n    velocity_norm_temp: float = 0.1\n    scaled_velocity_norm: Tensor = torch.exp(velocity_norm / velocity_norm_temp) \n\n    # Encourage humanoid to keep its footing\n    torso_height = torso_position[:, 2].view(-1, 1)\n    torso_height_temp: float = 0.25  # increased the relative contribution of torso height in the total reward\n    height_reward = torch.where(torso_height > 0.2, torch.ones_like(torso_height), torch.zeros_like(torso_height))\n    scaled_height_reward: Tensor = torch.exp(height_reward / torso_height_temp)\n\n    # Discourage sideways movement\n    sideways_velocity: Tensor = abs(velocity[:, 0]) + abs(velocity[:, 2])\n    sideways_velocity_temp: float = 0.1\n    sideways_penalty: Tensor = torch.exp(-sideways_velocity / sideways_velocity_temp)\n\n    # Minimize forces on joints\n    joint_force_temp: float = 0.1\n    joint_force_penalty: Tensor = torch.exp(-torch.norm(dof_force, dim=1) / joint_force_temp)\n\n    reward = scaled_velocity_norm + scaled_height_reward + sideways_penalty + joint_force_penalty\n    reward_components: Dict[str, Tensor] = {\"scaled_velocity_norm\": scaled_velocity_norm, \n                                            \"scaled_height_reward\": scaled_height_reward,\n                                            \"sideways_penalty\": sideways_penalty,\n                                            \"joint_force_penalty\": joint_force_penalty}\n\n    return reward, reward_components\n```",
        "score_rubric": {
            "criteria": "Does the response effectively adjust the reward function to address the issue of the humanoid frequently falling down, using relevant variables and transformations?",
            "score1_description": "The response fails to modify the reward function in a way that addresses the humanoid's stability issue, for example, by not changing any parameters or focusing solely on speed.",
            "score2_description": "The response makes minor adjustments to the reward function, such as slightly altering a temperature parameter, but these changes are insufficient to significantly impact the humanoid's stability.",
            "score3_description": "The response introduces a new component to the reward function, such as a penalty for sideways movement, but does not adequately balance this with other components, resulting in minimal improvement in stability.",
            "score4_description": "The response makes significant improvements to the reward function, like enhancing the height reward and adding a penalty for sideways movement, but one key aspect, such as minimizing joint forces, is overlooked.",
            "score5_description": "The response comprehensively revises the reward function, effectively incorporating increased height reward, penalties for sideways movement and excessive joint forces, leading to a noticeable improvement in the humanoid's stability."
        },
        "atomic_checklist": [
            "Does the response increase the relative importance of the torso height in the reward function to encourage stability?",
            "Is a new penalty introduced for sideways movement to discourage instability?",
            "Does the response include a penalty for excessive forces on joints, which could contribute to falls?",
            "Are all modifications to the reward function, such as scaling factors and penalties, clearly explained and justified?",
            "Does the response ensure that the reward function remains compatible with TorchScript, as required?",
            "Is the total reward calculated as a sum of all individual components, including both rewards and penalties?",
            "Does the response maintain the requirement that only attributes of the environment class definition are used in the reward function?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "planning",
        "task": "reward_modeling",
        "instance_idx": 1,
        "system_prompt": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs. As an example,\nthe reward function signature can be: @torch.jit.script\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    ...\n    return reward, {}\n\nSince the reward function will be decorated with @torch.jit.script,\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward,\n    (2) a dictionary of each individual reward component.\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
        "input": "### User: The Python environment is class FrankaCabinet(VecTask):\\n    \"\"\"Rest of the environment definition omitted.\"\"\"\\n    def compute_observations(self):\\n\\n        self.gym.refresh_actor_root_state_tensor(self.sim)\\n        self.gym.refresh_dof_state_tensor(self.sim)\\n        self.gym.refresh_rigid_body_state_tensor(self.sim)\\n\\n        hand_pos = self.rigid_body_states[:, self.hand_handle][:, 0:3]\\n        hand_rot = self.rigid_body_states[:, self.hand_handle][:, 3:7]\\n        drawer_pos = self.rigid_body_states[:, self.drawer_handle][:, 0:3]\\n        drawer_rot = self.rigid_body_states[:, self.drawer_handle][:, 3:7]\\n\\n        self.franka_grasp_rot[:], self.franka_grasp_pos[:], self.drawer_grasp_rot[:], self.drawer_grasp_pos[:] = \\\\\\n            compute_grasp_transforms(hand_rot, hand_pos, self.franka_local_grasp_rot, self.franka_local_grasp_pos,\\n                                     drawer_rot, drawer_pos, self.drawer_local_grasp_rot, self.drawer_local_grasp_pos\\n                                     )\\n\\n        self.franka_lfinger_pos = self.rigid_body_states[:, self.lfinger_handle][:, 0:3]\\n        self.franka_rfinger_pos = self.rigid_body_states[:, self.rfinger_handle][:, 0:3]\\n        self.franka_lfinger_rot = self.rigid_body_states[:, self.lfinger_handle][:, 3:7]\\n        self.franka_rfinger_rot = self.rigid_body_states[:, self.rfinger_handle][:, 3:7]\\n\\n        dof_pos_scaled = (2.0 * (self.franka_dof_pos - self.franka_dof_lower_limits)\\n                          / (self.franka_dof_upper_limits - self.franka_dof_lower_limits) - 1.0)\\n        to_target = self.drawer_grasp_pos - self.franka_grasp_pos\\n        self.obs_buf = torch.cat((dof_pos_scaled, self.franka_dof_vel * self.dof_vel_scale, to_target,\\n                                  self.cabinet_dof_pos[:, 3].unsqueeze(-1), self.cabinet_dof_vel[:, 3].unsqueeze(-1)), dim=-1)\\n\\n        return self.obs_buf\\n. Write a reward function for the following task: to open the cabinet door.\n\n### Assistant: Here's the reward function code:\n\nIt will be computed as a combination of two reward components:\n1) `distance_reward`: Which encourages the agent to move its hand closer to the cabinet door. This is defined as the negative L2 distance between the hand position and the target position (cabinet door position).\n2) `open_reward`: Which encourages the agent to open the cabinet door. This is defined as the current door position (assuming it ranges from 0: closed to 1: fully open), scaled by the door open scale since opening the door gives high reward.\n\nThe total reward is then a sum of these individual reward components after applying transformations using `torch.exp` function.\n\n```python\n@torch.jit.script\ndef compute_reward(franka_grasp_pos: torch.Tensor, drawer_grasp_pos: torch.Tensor, cabinet_dof_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    dist_to_target = torch.norm(franka_grasp_pos - drawer_grasp_pos, dim=-1)\n    \n    distance_reward: torch.Tensor = -dist_to_target\n    distance_temperature: float = 0.05\n    distance_reward_transformed: torch.Tensor = torch.exp(distance_reward / distance_temperature)\n    \n    door_open_scale: float = 1.0\n    open_reward: torch.Tensor = door_open_scale * cabinet_dof_pos[:, 3]\n    open_temperature: float = 0.1\n    open_reward_transformed: torch.Tensor = torch.exp(open_reward / open_temperature)\n\n    total_reward: torch.Tensor = distance_reward_transformed + open_reward_transformed\n\n    reward_components: Dict[str, torch.Tensor] = {\"distance_reward\": distance_reward_transformed, \"open_reward\": open_reward_transformed}\n    \n    return total_reward, reward_components\n```\n\nThis reward formula attempts to balance between pulling the cabinet door and reducing the distance between the agent's hand and the cabinet door. We've also applied exponential scaling to each reward component to control the contribution of each reward component to the total reward. \n\nThe distances and the degree of the opening of the cabinet drawer are normalized by `distance_temperature` and `open_temperature` to ensure numerical stability and make sure the rewards are within a certain scale.\n\nOn each step of the environment, you call this function with the current state variables as input, and use the returned total reward to update your policy.\n\n### User: We trained a RL policy using the provided reward function code and it seems that the arm is not fully opening the door.\n\n### Assistant: ",
        "reference_answer": "```python\n@torch.jit.script\ndef compute_reward(franka_grasp_pos: torch.Tensor, drawer_grasp_pos: torch.Tensor, cabinet_dof_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    dist_to_target = torch.norm(franka_grasp_pos - drawer_grasp_pos, dim=-1)\n    \n    distance_reward: torch.Tensor = -dist_to_target\n    distance_temperature: float = 0.05\n    distance_reward_transformed: torch.Tensor = torch.exp(distance_reward / distance_temperature)\n\n    door_open_scale: float = 1.0\n    # square the cabinet position as reward\n    open_reward: torch.Tensor = door_open_scale * torch.square(cabinet_dof_pos[:, 3])\n    open_temperature: float = 0.1\n    open_reward_transformed: torch.Tensor = torch.exp(open_reward / open_temperature)\n\n    total_reward: torch.Tensor = distance_reward_transformed + open_reward_transformed\n\n    reward_components: Dict[str, torch.Tensor] = {\"distance_reward\": distance_reward_transformed, \"open_reward\": open_reward_transformed}\n\n    return total_reward, reward_components\n```",
        "score_rubric": {
            "criteria": "Does the response effectively modify the reward function to encourage the agent to fully open the cabinet door, using appropriate variables and reward scaling?",
            "score1_description": "The response does not adjust the reward function to address the issue, maintaining the original components without modification.",
            "score2_description": "The response makes minimal adjustments to the reward function, such as slight changes to temperature parameters, which do not significantly impact the agent's behavior towards fully opening the door.",
            "score3_description": "The response introduces a minor but relevant modification to the reward function, like adjusting the scale of the open reward, but fails to adequately incentivize the full opening of the door.",
            "score4_description": "The response significantly modifies the reward function, for example, by squaring the door position component to emphasize fully opening the door, but lacks fine-tuning of parameters or additional components to ensure consistent behavior.",
            "score5_description": "The response comprehensively revises the reward function, effectively incorporating modifications such as squaring the door position component and fine-tuning reward scales, leading to a significant improvement in the agent's behavior to fully open the door."
        },
        "atomic_checklist": [
            "Does the response modify the `open_reward` component to emphasize the importance of fully opening the door, such as by squaring the door position?",
            "Is the `distance_reward` component maintained or adjusted to ensure the agent still prioritizes moving towards the door?",
            "Are the temperature parameters for the reward components adjusted to ensure numerical stability and appropriate scaling of rewards?",
            "Does the response introduce any new reward components or penalties to discourage partial opening or other undesired behaviors?",
            "Is the total reward calculation correctly updated to include all modified and new reward components?",
            "Does the response ensure that all modifications are compatible with TorchScript, as required by the environment?",
            "Are the modifications to the reward function justified based on their expected impact on the agent's behavior?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "planning",
        "task": "reward_modeling",
        "instance_idx": 2,
        "system_prompt": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs. As an example,\nthe reward function signature can be: @torch.jit.script\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    ...\n    return reward, {}\n\nSince the reward function will be decorated with @torch.jit.script,\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward,\n    (2) a dictionary of each individual reward component.\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
        "input": "### User: The Python environment is class FrankaCabinet(VecTask):\\n    \"\"\"Rest of the environment definition omitted.\"\"\"\\n    def compute_observations(self):\\n\\n        self.gym.refresh_actor_root_state_tensor(self.sim)\\n        self.gym.refresh_dof_state_tensor(self.sim)\\n        self.gym.refresh_rigid_body_state_tensor(self.sim)\\n\\n        hand_pos = self.rigid_body_states[:, self.hand_handle][:, 0:3]\\n        hand_rot = self.rigid_body_states[:, self.hand_handle][:, 3:7]\\n        drawer_pos = self.rigid_body_states[:, self.drawer_handle][:, 0:3]\\n        drawer_rot = self.rigid_body_states[:, self.drawer_handle][:, 3:7]\\n\\n        self.franka_grasp_rot[:], self.franka_grasp_pos[:], self.drawer_grasp_rot[:], self.drawer_grasp_pos[:] = \\\\\\n            compute_grasp_transforms(hand_rot, hand_pos, self.franka_local_grasp_rot, self.franka_local_grasp_pos,\\n                                     drawer_rot, drawer_pos, self.drawer_local_grasp_rot, self.drawer_local_grasp_pos\\n                                     )\\n\\n        self.franka_lfinger_pos = self.rigid_body_states[:, self.lfinger_handle][:, 0:3]\\n        self.franka_rfinger_pos = self.rigid_body_states[:, self.rfinger_handle][:, 0:3]\\n        self.franka_lfinger_rot = self.rigid_body_states[:, self.lfinger_handle][:, 3:7]\\n        self.franka_rfinger_rot = self.rigid_body_states[:, self.rfinger_handle][:, 3:7]\\n\\n        dof_pos_scaled = (2.0 * (self.franka_dof_pos - self.franka_dof_lower_limits)\\n                          / (self.franka_dof_upper_limits - self.franka_dof_lower_limits) - 1.0)\\n        to_target = self.drawer_grasp_pos - self.franka_grasp_pos\\n        self.obs_buf = torch.cat((dof_pos_scaled, self.franka_dof_vel * self.dof_vel_scale, to_target,\\n                                  self.cabinet_dof_pos[:, 3].unsqueeze(-1), self.cabinet_dof_vel[:, 3].unsqueeze(-1)), dim=-1)\\n\\n        return self.obs_buf\\n. Write a reward function for the following task: to open the cabinet door.\n\n### Assistant: Here's the reward function code:\n\nIt will be computed as a combination of two reward components:\n1) `distance_reward`: Which encourages the agent to move its hand closer to the cabinet door. This is defined as the negative L2 distance between the hand position and the target position (cabinet door position).\n2) `open_reward`: Which encourages the agent to open the cabinet door. This is defined as the current door position (assuming it ranges from 0: closed to 1: fully open), scaled by the door open scale since opening the door gives high reward.\n\nThe total reward is then a sum of these individual reward components after applying transformations using `torch.exp` function.\n\n```python\n@torch.jit.script\ndef compute_reward(franka_grasp_pos: torch.Tensor, drawer_grasp_pos: torch.Tensor, cabinet_dof_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    dist_to_target = torch.norm(franka_grasp_pos - drawer_grasp_pos, dim=-1)\n    \n    distance_reward: torch.Tensor = -dist_to_target\n    distance_temperature: float = 0.05\n    distance_reward_transformed: torch.Tensor = torch.exp(distance_reward / distance_temperature)\n    \n    door_open_scale: float = 1.0\n    open_reward: torch.Tensor = door_open_scale * cabinet_dof_pos[:, 3]\n    open_temperature: float = 0.1\n    open_reward_transformed: torch.Tensor = torch.exp(open_reward / open_temperature)\n\n    total_reward: torch.Tensor = distance_reward_transformed + open_reward_transformed\n\n    reward_components: Dict[str, torch.Tensor] = {\"distance_reward\": distance_reward_transformed, \"open_reward\": open_reward_transformed}\n    \n    return total_reward, reward_components\n```\n\nThis reward formula attempts to balance between pulling the cabinet door and reducing the distance between the agent's hand and the cabinet door. We've also applied exponential scaling to each reward component to control the contribution of each reward component to the total reward. \n\nThe distances and the degree of the opening of the cabinet drawer are normalized by `distance_temperature` and `open_temperature` to ensure numerical stability and make sure the rewards are within a certain scale.\n\nOn each step of the environment, you call this function with the current state variables as input, and use the returned total reward to update your policy.\n\n### User: We trained a RL policy using the provided reward function code and it seems that the arm is not able to properly grasp the drawer.\n\n### Assistant: ",
        "reference_answer": "```python\n@torch.jit.script\ndef compute_reward(franka_grasp_pos: torch.Tensor, drawer_grasp_pos: torch.Tensor, \n                   cabinet_dof_pos: torch.Tensor, franka_rfinger_pos: torch.Tensor, \n                   franka_lfinger_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    \n    # Reward for moving hand to the drawer handle\n    dist_to_target = torch.norm(franka_grasp_pos - drawer_grasp_pos, dim=-1)\n    distance_reward: torch.Tensor = -dist_to_target\n    distance_temperature: float = 0.05\n    distance_reward_transformed: torch.Tensor = torch.exp(distance_reward / distance_temperature)\n\n    # Reward for opening the cabinet\n    door_open_scale: float = 1.0\n    open_reward: torch.Tensor = door_open_scale * torch.square(cabinet_dof_pos[:, 3])\n    open_temperature: float = 0.1\n    open_reward_transformed: torch.Tensor = torch.exp(open_reward / open_temperature)\n\n    # Reward for successful grasping\n    fingers_distance = torch.norm(franka_rfinger_pos - franka_lfinger_pos, dim=-1)\n    grasp_reward: torch.Tensor = -fingers_distance\n    grasp_temperature: float = 0.03\n    grasp_reward_transformed: torch.Tensor = torch.exp(grasp_reward / grasp_temperature)\n\n    # Total Reward\n    total_reward: torch.Tensor = distance_reward_transformed + open_reward_transformed + grasp_reward_transformed\n\n    reward_components: Dict[str, torch.Tensor] = {\n        \"distance_reward\": distance_reward_transformed, \n        \"open_reward\": open_reward_transformed, \n        \"grasp_reward\": grasp_reward_transformed\n    }\n\n    return total_reward, reward_components\n```",
        "score_rubric": {
            "criteria": "Does the response effectively adapt the reward function to encourage the agent to properly grasp the drawer, using relevant variables and reward transformations?",
            "score1_description": "The response does not modify or introduce new reward components to address the grasping issue, maintaining the original reward structure without addressing the problem.",
            "score2_description": "The response makes minimal adjustments, such as minor tweaks to existing reward components, which do not significantly contribute to solving the grasping issue.",
            "score3_description": "The response introduces a new reward component related to grasping, such as a penalty for finger distance, but fails to integrate it effectively with the existing reward structure, leading to limited improvement.",
            "score4_description": "The response effectively introduces a new reward component to encourage proper grasping and makes adjustments to integrate this with existing components, resulting in noticeable improvements in grasping behavior.",
            "score5_description": "The response comprehensively revises the reward function, successfully integrating a new grasping reward component with adjustments to existing components, leading to significant improvements in the agent's ability to grasp the drawer."
        },
        "atomic_checklist": [
            "Does the response introduce a new reward component specifically aimed at improving the grasping action, such as a penalty based on the distance between the robot's fingers?",
            "Is the new grasping reward component effectively integrated with existing reward components like the `distance_reward` and `open_reward`, ensuring a balanced incentive structure?",
            "Are the temperature parameters for new and existing reward components adjusted to ensure appropriate scaling and numerical stability?",
            "Does the response maintain the requirement for compatibility with TorchScript, as specified by the environment's constraints?",
            "Is the total reward calculation updated to include the new grasping reward component along with the existing components?",
            "Does the response provide a clear rationale for the introduction and scaling of the new grasping reward component, explaining its expected impact on the agent's behavior?",
            "Are all modifications to the reward function justified in terms of their relevance to the task and the specific issue of grasping the drawer?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "planning",
        "task": "reward_modeling",
        "instance_idx": 3,
        "system_prompt": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs. As an example,\nthe reward function signature can be: @torch.jit.script\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    ...\n    return reward, {}\n\nSince the reward function will be decorated with @torch.jit.script,\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward,\n    (2) a dictionary of each individual reward component.\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
        "input": "### User: The Python environment is class Humanoid(VecTask):\\\\n    \\\"\\\"\\\"Rest of the environment definition omitted.\\\"\\\"\\\"\\\\n    def compute_observations(self):\\\\n        self.gym.refresh_dof_state_tensor(self.sim)\\\\n        self.gym.refresh_actor_root_state_tensor(self.sim)\\\\n\\\\n        self.gym.refresh_force_sensor_tensor(self.sim)\\\\n        self.gym.refresh_dof_force_tensor(self.sim)\\\\n        self.obs_buf[:], self.potentials[:], self.prev_potentials[:], self.up_vec[:], self.heading_vec[:] = compute_humanoid_observations(\\\\n            self.obs_buf, self.root_states, self.targets, self.potentials,\\\\n            self.inv_start_rot, self.dof_pos, self.dof_vel, self.dof_force_tensor,\\\\n            self.dof_limits_lower, self.dof_limits_upper, self.dof_vel_scale,\\\\n            self.vec_sensor_tensor, self.actions, self.dt, self.contact_force_scale, self.angular_velocity_scale,\\\\n            self.basis_vec0, self.basis_vec1)\\\\n\\\\ndef compute_humanoid_observations(obs_buf, root_states, targets, potentials, inv_start_rot, dof_pos, dof_vel,\\\\n                                  dof_force, dof_limits_lower, dof_limits_upper, dof_vel_scale,\\\\n                                  sensor_force_torques, actions, dt, contact_force_scale, angular_velocity_scale,\\\\n                                  basis_vec0, basis_vec1):\\\\n    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Tensor, Tensor, float, float, float, Tensor, Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]\\\\n\\\\n    torso_position = root_states[:, 0:3]\\\\n    torso_rotation = root_states[:, 3:7]\\\\n    velocity = root_states[:, 7:10]\\\\n    ang_velocity = root_states[:, 10:13]\\\\n\\\\n    to_target = targets - torso_position\\\\n    to_target[:, 2] = 0\\\\n\\\\n    prev_potentials_new = potentials.clone()\\\\n    potentials = -torch.norm(to_target, p=2, dim=-1) / dt\\\\n\\\\n    torso_quat, up_proj, heading_proj, up_vec, heading_vec = compute_heading_and_up(\\\\n        torso_rotation, inv_start_rot, to_target, basis_vec0, basis_vec1, 2)\\\\n\\\\n    vel_loc, angvel_loc, roll, pitch, yaw, angle_to_target = compute_rot(\\\\n        torso_quat, velocity, ang_velocity, targets, torso_position)\\\\n\\\\n    roll = normalize_angle(roll).unsqueeze(-1)\\\\n    yaw = normalize_angle(yaw).unsqueeze(-1)\\\\n    angle_to_target = normalize_angle(angle_to_target).unsqueeze(-1)\\\\n    dof_pos_scaled = unscale(dof_pos, dof_limits_lower, dof_limits_upper)\\\\n\\\\n    obs = torch.cat((torso_position[:, 2].view(-1, 1), vel_loc, angvel_loc * angular_velocity_scale,\\\\n                     yaw, roll, angle_to_target, up_proj.unsqueeze(-1), heading_proj.unsqueeze(-1),\\\\n                     dof_pos_scaled, dof_vel * dof_vel_scale, dof_force * contact_force_scale,\\\\n                     sensor_force_torques.view(-1, 12) * contact_force_scale, actions), dim=-1)\\\\n\\\\n    return obs, potentials, prev_potentials_new, up_vec, heading_vec\\\\n\\\\n\\\\n\\\\n. Write a reward function for the following task: to make the humanoid run as fast as possible.\n\n### Assistant: ```python\nimport torch\nfrom torch import Tensor\nfrom typing import Tuple, Dict\n\n@torch.jit.script\ndef compute_reward(velocity: Tensor, up_vec: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n    # Reward components\n    forward_velocity = velocity[:, 0]  # Assuming first component of velocity is forward\n    \n    # Temperature parameters\n    forward_velocity_temp = 0.1\n    upright_penalty_temp = 0.1\n    \n    # We want to maximize the forward speed while penalizing any deviation from being upright\n    speed_reward = torch.exp(forward_velocity / forward_velocity_temp)\n    \n    # Calculate the upright reward. The up_vec's last component should be close to 1 when upright\n    # Assuming up_vec's last component corresponds to the 'z' component in world space\n    upright_reward = torch.exp((up_vec[:, 2] - 1.0) / upright_penalty_temp)\n    \n    # Combine rewards\n    total_reward = speed_reward + upright_reward\n    \n    # Create a dictionary to hold the individual components of the reward\n    reward_components = {\n        \"speed_reward\": speed_reward,\n        \"upright_reward\": upright_reward\n    }\n    \n    return total_reward, reward_components\n\n# The following dummy tensors are placeholders for this example\ndummy_velocity = torch.tensor([[1, 0, 0]], dtype=torch.float32)  # Placeholder for actual velocity\ndummy_up_vec = torch.tensor([[0, 0, 1]], dtype=torch.float32)  # Placeholder for actual up_vec\n\n# Example call to compute_reward function\nreward, reward_parts = compute_reward(dummy_velocity, dummy_up_vec)\nprint(f\"Reward: {reward}, Reward Parts: {reward_parts}\")\n```\n\n### User: We trained a RL policy using the provided reward function code and it seems that the humanoid is not doing anything.\n\n### Assistant: ",
        "reference_answer": "```python\nimport torch\nfrom torch import Tensor\nfrom typing import Tuple, Dict\n\n@torch.jit.script\ndef compute_reward(velocity: Tensor, up_vec: Tensor, angular_velocity: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n    # Constants for reward computation\n    forward_velocity_temp = 1.0  # Adjusted temperature for sensitivity\n    upright_scale = 0.5  # Scaling factor for upright reward\n    ang_velocity_scale = 0.05  # Scaling factor for penalizing high angular velocity\n    \n    # Calculate the forward speed reward (assuming the first index is the forward direction)\n    forward_velocity_reward = torch.clamp(velocity[:, 0], min=0.0)  # Only reward forward movement\n    speed_reward = torch.exp(forward_velocity_reward / forward_velocity_temp)\n    \n    # Calculate upright reward (assuming the up_vec's last component indicates uprightness)\n    uprightness = up_vec[:, 2]\n    upright_penalty = -torch.abs(uprightness - 1.0) * upright_scale  # Penalize deviation from being upright\n    upright_reward = torch.exp(upright_penalty)\n    \n    # We can also penalize high angular velocity to encourage smooth movement\n    angular_velocity_penalty = -torch.norm(angular_velocity, dim=1) * ang_velocity_scale\n    ang_vel_reward = torch.exp(angular_velocity_penalty)\n    \n    # Combine rewards\n    total_reward = speed_reward + upright_reward + ang_vel_reward\n    \n    # Create a dictionary to hold the individual components of the reward\n    reward_components = {\n        \"speed_reward\": speed_reward,\n        \"upright_reward\": upright_reward,\n        \"angular_velocity_penalty\": ang_vel_reward\n    }\n    \n    return total_reward.mean(), reward_components\n\n# The following dummy tensors are placeholders for this example and should be replaced with real observations\ndummy_velocity = torch.tensor([[0.5, 0, 0]], dtype=torch.float32)  # Placeholder for actual forward velocity\ndummy_up_vec = torch.tensor([[0, 0, 1]], dtype=torch.float32)      # Placeholder for actual up vector\ndummy_angular_velocity = torch.tensor([[0, 0, 0]], dtype=torch.float32)  # Placeholder for actual angular velocity\n\n# Example call to compute_reward function\nreward, reward_parts = compute_reward(dummy_velocity, dummy_up_vec, dummy_angular_velocity)\nprint(f\"Reward: {reward}, Reward Parts: {reward_parts}\")\n```",
        "score_rubric": {
            "criteria": "Does the response effectively design a reward function that encourages the humanoid to run fast while maintaining an upright posture, and possibly penalizes excessive angular velocity?",
            "score1_description": "The response fails to address key components of the task, missing out on rewards for forward motion and upright posture, and does not mention angular velocity.",
            "score2_description": "The response includes a reward for forward motion but neglects upright posture and angular velocity, leading to an incomplete solution for the task.",
            "score3_description": "The response provides rewards for both forward motion and upright posture but lacks a penalty for excessive angular velocity, resulting in a potentially unstable running behavior.",
            "score4_description": "The response includes well-designed rewards for forward motion and upright posture, and introduces a penalty for angular velocity, but the integration of these components lacks balance or fine-tuning.",
            "score5_description": "The response expertly crafts a reward function with balanced rewards for forward motion and upright posture, along with a well-integrated penalty for excessive angular velocity, effectively promoting stable and fast running behavior."
        },
        "atomic_checklist": [
            "Does the response include a reward component for forward motion that encourages the humanoid to run fast?",
            "Is there a reward component for maintaining an upright posture to ensure stability while running?",
            "Does the response introduce a penalty for excessive angular velocity to discourage unstable movements?",
            "Are the reward components scaled or transformed (e.g., using exponential functions) to control their influence on the total reward?",
            "Does the response ensure that the reward function is compatible with TorchScript, adhering to the environment's requirements?",
            "Are temperature parameters or scaling factors used thoughtfully to balance the contributions of different reward components?",
            "Is the total reward calculated as a coherent sum of the individual components, effectively promoting the desired behavior?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "planning",
        "task": "reward_modeling",
        "instance_idx": 4,
        "system_prompt": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs. As an example,\nthe reward function signature can be: @torch.jit.script\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    ...\n    return reward, {}\n\nSince the reward function will be decorated with @torch.jit.script,\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward,\n    (2) a dictionary of each individual reward component.\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
        "input": "### User: The Python environment is class BallBalance(VecTask):\n    \"\"\"Rest of the environment definition omitted.\"\"\"\n    def compute_observations(self):\n\n        actuated_dof_indices = torch.tensor([1, 3, 5], device=self.device)\n\n        self.obs_buf[..., 0:3] = self.dof_positions[..., actuated_dof_indices]\n        self.obs_buf[..., 3:6] = self.dof_velocities[..., actuated_dof_indices]\n        self.obs_buf[..., 6:9] = self.ball_positions\n        self.obs_buf[..., 9:12] = self.ball_linvels\n        self.obs_buf[..., 12:15] = self.sensor_forces[..., 0] / 20  # !!! lousy normalization\n        self.obs_buf[..., 15:18] = self.sensor_torques[..., 0] / 20  # !!! lousy normalization\n        self.obs_buf[..., 18:21] = self.sensor_torques[..., 1] / 20  # !!! lousy normalization\n        self.obs_buf[..., 21:24] = self.sensor_torques[..., 2] / 20  # !!! lousy normalization\n\n        return self.obs_buf\n. Write a reward function for the following task: to keep the ball on the table top without falling.\n\n### Assistant: ```python\nimport torch\nfrom torch import Tensor\nfrom typing import Tuple, Dict\n\n@torch.jit.script\ndef compute_reward(ball_positions: Tensor, \n                   ball_linvels: Tensor, \n                   sensor_forces: Tensor, \n                   sensor_torques: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n    # Reward components temperatures\n    distance_temp: float = 0.1\n    velocity_temp: float = 0.1\n    force_temp: float = 0.01\n    torque_temp: float = 0.01\n\n    # Compute the distance of the ball from the center of the table\n    center = torch.tensor([0.0, 0.0, 0.0], device=ball_positions.device)\n    distance = torch.norm(ball_positions - center, dim=-1)\n\n    # Compute the velocity of the ball\n    velocity = torch.norm(ball_linvels, dim=-1)\n\n    # Compute the force and torque applied by the actuators\n    force = torch.norm(sensor_forces, dim=-1)\n    torque = torch.norm(sensor_torques, dim=-1)\n\n    # Calculate individual negative rewards (we want to minimize these quantities)\n    distance_reward = -torch.exp(distance / distance_temp)\n    velocity_reward = -torch.exp(velocity / velocity_temp)\n    force_reward = -torch.exp(force / force_temp)\n    torque_reward = -torch.exp(torque / torque_temp)\n\n    # Sum up the individual rewards to get the total reward\n    total_reward = distance_reward + velocity_reward + force_reward + torque_reward\n\n    # Create a dictionary for the individual reward components\n    reward_components: Dict[str, Tensor] = {\n        'distance_reward': distance_reward,\n        'velocity_reward': velocity_reward,\n        'force_reward': force_reward,\n        'torque_reward': torque_reward\n    }\n\n    return total_reward, reward_components\n\n# Example call to the reward function\n# Note: Replace `env` with the actual environment object\n# total_reward, rewards_dict = compute_reward(env.ball_positions, env.ball_linvels, env.sensor_forces, env.sensor_torques)\n```\n\n### User: We trained a RL policy using the provided reward function code and but it spins the table causing the ball to leave.\n\n### Assistant: ",
        "reference_answer": "```python\nimport torch\nfrom torch import Tensor\nfrom typing import Tuple, Dict\n\n@torch.jit.script\ndef compute_reward(ball_positions: Tensor, \n                   ball_linvels: Tensor, \n                   sensor_forces: Tensor, \n                   sensor_torques: Tensor,\n                   table_angvels: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n    # Reward components temperatures\n    distance_temp: float = 0.1\n    velocity_temp: float = 0.1\n    force_temp: float = 0.01\n    torque_temp: float = 0.01\n    angvel_temp: float = 0.1\n\n    # Threshold for considering the ball to be off the table\n    off_table_threshold: float = 1.0\n    \n    # Compute the distance of the ball from the center of the table\n    center = torch.tensor([0.0, 0.0, 0.0], device=ball_positions.device)\n    distance = torch.norm(ball_positions - center, dim=-1)\n\n    # Compute the velocity of the ball\n    velocity = torch.norm(ball_linvels, dim=-1)\n\n    # Compute the angular velocity of the table\n    angvel = torch.norm(table_angvels, dim=-1)\n\n    # Compute the force and torque applied by the actuators\n    force = torch.norm(sensor_forces, dim=-1)\n    torque = torch.norm(sensor_torques, dim=-1)\n\n    # Calculate individual negative rewards (we want to minimize these quantities)\n    distance_reward = -torch.exp(distance / distance_temp)\n    velocity_reward = -torch.exp(velocity / velocity_temp)\n    angvel_reward = -torch.exp(angvel / angvel_temp)\n    force_reward = -torch.exp(force / force_temp)\n    torque_reward = -torch.exp(torque / torque_temp)\n\n    # If the ball is off the table, apply a large negative reward\n    off_table_penalty = torch.where(\n        distance > off_table_threshold,\n        torch.tensor(-100.0, device=ball_positions.device),\n        torch.tensor(0.0, device=ball_positions.device)\n    )\n\n    # Sum up the individual rewards to get the total reward\n    total_reward = distance_reward + velocity_reward + angvel_reward + force_reward + torque_reward + off_table_penalty\n\n    # Create a dictionary for the individual reward components\n    reward_components: Dict[str, Tensor] = {\n        'distance_reward': distance_reward,\n        'velocity_reward': velocity_reward,\n        'angvel_reward': angvel_reward,\n        'force_reward': force_reward,\n        'torque_reward': torque_reward,\n        'off_table_penalty': off_table_penalty\n    }\n\n    return total_reward, reward_components\n\n# Example call to the reward function\n# Note: Replace `env` with the actual environment object\n# total_reward, rewards_dict = compute_reward(env.ball_positions, env.ball_linvels, env.sensor_forces, env.sensor_torques, env.table_angvels)\n```",
        "score_rubric": {
            "criteria": "Does the response effectively design a reward function that balances the ball on the table top, minimizing excessive table movements and penalizing the ball falling off?",
            "score1_description": "The response fails to address the task's key aspects, missing rewards for ball stability and penalties for excessive table movements or the ball falling off.",
            "score2_description": "The response includes a basic reward for ball stability but neglects to adequately penalize excessive table movements or the ball falling off, leading to an incomplete solution.",
            "score3_description": "The response provides rewards for ball stability and introduces penalties for excessive table movements, but lacks a clear penalty for the ball falling off, resulting in suboptimal performance.",
            "score4_description": "The response includes well-designed rewards for ball stability and penalties for excessive table movements, with a basic penalty for the ball falling off, but the integration of these components lacks balance or fine-tuning.",
            "score5_description": "The response expertly crafts a reward function with balanced rewards for ball stability, penalties for excessive table movements, and a well-integrated penalty for the ball falling off, effectively promoting the desired behavior."
        },
        "atomic_checklist": [
            "Does the response include a reward component for keeping the ball close to the center of the table?",
            "Is there a penalty for excessive velocity of the ball to discourage rapid movements?",
            "Does the response introduce a penalty for excessive angular velocity of the table to discourage spinning?",
            "Are the reward components scaled or transformed (e.g., using exponential functions) to control their influence on the total reward?",
            "Does the response ensure that the reward function is compatible with TorchScript, adhering to the environment's requirements?",
            "Are temperature parameters or scaling factors used thoughtfully to balance the contributions of different reward components?",
            "Is there a significant penalty for the ball falling off the table to strongly discourage this outcome?",
            "Is the total reward calculated as a coherent sum of the individual components, effectively promoting the desired behavior of balancing the ball on the table?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "planning",
        "task": "reward_modeling",
        "instance_idx": 5,
        "system_prompt": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs. As an example,\nthe reward function signature can be: @torch.jit.script\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    ...\n    return reward, {}\n\nSince the reward function will be decorated with @torch.jit.script,\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward,\n    (2) a dictionary of each individual reward component.\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.",
        "input": "### User: The Python environment is class SpaceExplorer(VecTask):\n    \"\"\"Rest of the environment definition omitted.\"\"\"\n    def compute_observations(self):\n\n        self.spacecraft_pos = torch.randn((1, 3), device=self.device) # Spacecraft position\n        self.planet_pos = self.target_planet_position # Target planet position\n        self.obstacle_positions = torch.randn((5, 3), device=self.device) # Random obstacle positions\n        self.fuel_level = torch.tensor([0.75], device=self.device) # Fuel level (0 to 1)\n\n        self.obs_buf = torch.cat((self.spacecraft_pos, self.planet_pos, self.obstacle_positions.flatten(), self.fuel_level), dim=-1)\n        return self.obs_buf\n. Write a reward function for the following task: to land on the target planet using the shortest route while avoiding obstacles and conserving fuel.",
        "reference_answer": "```python\nimport torch\nfrom torch import Tensor\nfrom typing import Tuple, Dict\n\n@torch.jit.script\ndef compute_reward(spacecraft_pos: Tensor, planet_pos: Tensor, obstacle_positions: Tensor, fuel_level: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n    # Constants for reward computation\n    proximity_temp: float = 0.1\n    fuel_temp: float = 0.05\n    obstacle_temp: float = 0.05\n\n    # Compute the distance to the target planet\n    distance_to_planet = torch.norm(planet_pos - spacecraft_pos, dim=-1)\n\n    # Calculate fuel conservation bonus\n    fuel_bonus = torch.exp(fuel_level / fuel_temp)\n\n    # Calculate the minimum distance to any obstacle\n    min_dist_to_obstacle = torch.cdist(spacecraft_pos.unsqueeze(0), obstacle_positions).min()\n    obstacle_penalty = -torch.exp(-min_dist_to_obstacle / obstacle_temp)\n\n    # Calculated individual rewards and penalties\n    proximity_reward = -torch.exp(distance_to_planet / proximity_temp)\n\n    # Total Reward\n    total_reward = proximity_reward + fuel_bonus + obstacle_penalty\n\n    # Reward components dictionary\n    reward_components: Dict[str, Tensor] = {\n        'proximity_reward': proximity_reward,\n        'fuel_bonus': fuel_bonus,\n        'obstacle_penalty': obstacle_penalty\n    }\n\n    return total_reward, reward_components\n\n# Placeholder tensors should be replaced with the actual environment observations\n# Example call to compute_reward function\n# reward, rewards_dict = compute_reward(dummy_spacecraft_pos, dummy_planet_pos, dummy_obstacle_positions, dummy_fuel_level)\n```",
        "score_rubric": {
            "criteria": "Does the response effectively design a reward function that leads the spacecraft to land on the target planet efficiently, avoiding obstacles and conserving fuel, using relevant variables and reward transformations?",
            "score1_description": "The response fails to address key components of the task, missing rewards for efficient route planning, obstacle avoidance, or fuel conservation.",
            "score2_description": "The response includes basic rewards for proximity to the planet but neglects comprehensive strategies for obstacle avoidance or fuel conservation, leading to an incomplete solution.",
            "score3_description": "The response provides rewards for both proximity to the planet and some form of obstacle avoidance or fuel conservation, but lacks a balanced or effective integration of these components.",
            "score4_description": "The response includes well-designed rewards for proximity to the planet, obstacle avoidance, and fuel conservation, but could improve in balance among these components or precision in penalties.",
            "score5_description": "The response expertly crafts a comprehensive reward function with perfectly balanced rewards for efficient route to the planet, sophisticated strategies for obstacle avoidance, and strong incentives for fuel conservation, effectively promoting the desired behavior."
        },
        "atomic_checklist": [
            "Does the response include a reward component for reducing the distance to the target planet?",
            "Is there a significant penalty for colliding with obstacles to promote route optimization?",
            "Does the response incorporate a reward for fuel conservation, penalizing inefficient fuel use?",
            "Are the reward components scaled or transformed to ensure their influence on the total reward is well controlled?",
            "Does the response ensure that the reward function is compatible with TorchScript, adhering to the environment's requirements?",
            "Are temperature parameters or scaling factors used thoughtfully to balance the contributions of different reward components?",
            "Is the total reward calculated as a coherent sum of the individual components, effectively promoting the desired behavior of landing on the planet efficiently?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "planning",
        "task": "reward_modeling",
        "instance_idx": 6,
        "system_prompt": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs. As an example,\nthe reward function signature can be: @torch.jit.script\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    ...\n    return reward, {}\n\nSince the reward function will be decorated with @torch.jit.script,\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward,\n    (2) a dictionary of each individual reward component.\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.",
        "input": "### User: The Python environment is class ArcticExplorer(VecTask):\n    \"\"\"Rest of the environment definition omitted.\"\"\"\n    def compute_observations(self):\n\n        self.explorer_pos = torch.randn((1, 3), device=self.device) # Explorer position\n        self.base_camp_pos = torch.tensor([0.0, 0.0, 0.0], device=self.device) # Base camp position\n        self.sample_locations = torch.randn((3, 3), device=self.device) # Sample location positions\n        self.temperature = torch.tensor([-30.0], device=self.device) # Ambient temperature\n\n        self.obs_buf = torch.cat((self.explorer_pos, self.base_camp_pos, self.sample_locations.flatten(), self.temperature), dim=-1)\n        return self.obs_buf\n. Write a reward function for the following task: to collect samples from specified locations and return to base camp before the explorer gets too cold.",
        "reference_answer": "```python\nimport torch\nfrom torch import Tensor\nfrom typing import Tuple, Dict\n\n@torch.jit.script\ndef compute_reward(explorer_pos: Tensor, base_camp_pos: Tensor, sample_locations: Tensor, temperature: Tensor, samples_collected: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n    # Constants for reward computation\n    sample_temp: float = 0.1\n    return_temp: float = 0.05\n    cold_exposure_temp: float = 0.02\n\n    # Compute proximity to samples and base camp\n    distance_to_samples = torch.cdist(explorer_pos.unsqueeze(0), sample_locations).min()\n    distance_to_base = torch.norm(base_camp_pos - explorer_pos, dim=-1)\n\n    # Calculate rewards and penalties\n    sample_collection_reward = -torch.exp(distance_to_samples / sample_temp) * samples_collected\n    return_reward = -torch.exp(distance_to_base / return_temp) * samples_collected\n    cold_exposure_penalty = -torch.exp(temperature / cold_exposure_temp)\n\n    # Total Reward\n    total_reward = sample_collection_reward + return_reward + cold_exposure_penalty\n\n    # Reward components dictionary\n    reward_components: Dict[str, Tensor] = {\n        'sample_collection_reward': sample_collection_reward,\n        'return_reward': return_reward,\n        'cold_exposure_penalty': cold_exposure_penalty\n    }\n\n    return total_reward, reward_components\n\n# Placeholder tensors should be replaced with the actual environment observations\n# Example call to compute_reward function\n# reward, rewards_dict = compute_reward(dummy_explorer_pos, dummy_base_camp_pos, dummy_sample_locations, dummy_temperature, dummy_samples_collected)\n```",
        "score_rubric": {
            "criteria": "Does the response effectively design a reward function that leads the explorer to collect samples and return to base camp quickly, minimizing exposure to cold temperatures, using relevant variables and reward transformations?",
            "score1_description": "The response fails to provide incentives for sample collection, base camp return, or considerations for minimizing cold exposure.",
            "score2_description": "The response includes rewards for either sample collection or returning to base camp, but neglects comprehensive strategies for minimizing cold exposure, leading to an incomplete solution.",
            "score3_description": "The response provides rewards for sample collection and base camp return, with some form of cold exposure minimization, but lacks a balanced integration of these components.",
            "score4_description": "The response contains well-thought-out rewards for sample collection, returning to base camp, and methods to minimize cold exposure, but could improve in balance and precision.",
            "score5_description": "The response expertly crafts a comprehensive reward function with balanced incentives for efficient sample collection, prompt return to base camp, and effective strategies for minimizing cold exposure, promoting the desired behavior efficiently."
        },
        "atomic_checklist": [
            "Does the response include a reward component for reaching each sample location?",
            "Is there a significant reward for returning to base camp with all samples collected?",
            "Does the response incorporate a strategy for minimizing cold exposure through rewards or penalties?",
            "Are the reward components scaled or transformed to ensure their influence on the total reward is well controlled?",
            "Does the response ensure that the reward function is compatible with TorchScript, adhering to the environment's requirements?",
            "Are temperature parameters or scaling factors used thoughtfully to balance the contributions of different reward components?",
            "Is the total reward calculated as a coherent sum of the individual components, effectively promoting the desired behavior of sample collection and safe return within the constraints?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "planning",
        "task": "reward_modeling",
        "instance_idx": 7,
        "system_prompt": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs. As an example,\nthe reward function signature can be: @torch.jit.script\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    ...\n    return reward, {}\n\nSince the reward function will be decorated with @torch.jit.script,\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward,\n    (2) a dictionary of each individual reward component.\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.",
        "input": "### User: The Python environment is class DeepSeaDiscovery(VecTask):\n    \"\"\"Rest of the environment definition omitted.\"\"\"\n    def compute_observations(self):\n\n        self.submarine_pos = torch.randn((1, 3), device=self.device) # Submarine position\n        self.treasure_pos = torch.randn((1, 3), device=self.device) # Treasure position\n        self.obstacle_positions = torch.randn((10, 3), device=self.device) # Random obstacle positions\n        self.oxygen_level = torch.tensor([1.0], device=self.device) # Full oxygen tank\n\n        self.obs_buf = torch.cat((self.submarine_pos, self.treasure_pos, self.obstacle_positions.flatten(), self.oxygen_level), dim=-1)\n        return self.obs_buf\n. Write a reward function for the following task: to discover underwater treasures while navigating through obstacles and managing oxygen levels.",
        "reference_answer": "```python\nimport torch\nfrom torch import Tensor\nfrom typing import Tuple, Dict\n\n@torch.jit.script\ndef compute_reward(submarine_pos: Tensor, treasure_pos: Tensor, obstacle_positions: Tensor, oxygen_level: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n    # Constants for reward calculation\n    treasure_temp: float = 0.1\n    oxygen_temp: float = 0.05\n    obstacle_temp: float = 0.05\n\n    # Compute the distance to the treasure\n    distance_to_treasure = torch.norm(treasure_pos - submarine_pos, dim=-1)\n\n    # Reward for oxygen conservation\n    oxygen_reward = torch.exp(oxygen_level / oxygen_temp)\n\n    # Penalty for proximity to obstacles\n    min_dist_to_obstacle = torch.cdist(submarine_pos.unsqueeze(0), obstacle_positions).min()\n    obstacle_penalty = -torch.exp(-min_dist_to_obstacle / obstacle_temp)\n\n    # Construct individual rewards\n    treasure_proximity_reward = -torch.exp(distance_to_treasure / treasure_temp)\n\n    # Total Reward\n    total_reward = treasure_proximity_reward + oxygen_reward + obstacle_penalty\n\n    # Reward components dictionary\n    reward_components: Dict[str, Tensor] = {\n        'treasure_proximity_reward': treasure_proximity_reward,\n        'oxygen_reward': oxygen_reward,\n        'obstacle_penalty': obstacle_penalty\n    }\n\n    return total_reward, reward_components\n\n# Placeholder tensors should be replaced with the actual environment observations\n# Example call to compute_reward function\n# reward, rewards_dict = compute_reward(dummy_submarine_pos, dummy_treasure_pos, dummy_obstacle_positions, dummy_oxygen_level)\n```",
        "score_rubric": {
            "criteria": "Does the response effectively design a reward function that navigates the submarine to discover treasures efficiently, avoiding obstacles and managing oxygen levels, using relevant variables and reward transformations?",
            "score1_description": "The response fails to incentivize treasure discovery, obstacle avoidance, or oxygen level management.",
            "score2_description": "The response includes a basic reward for treasure proximity but neglects to address strategies for obstacle avoidance or oxygen level management, resulting in a partial solution.",
            "score3_description": "The response supplies rewards for treasure proximity and some means of obstacle avoidance or oxygen conservation, yet lacks a cohesive strategy for integrating these factors together.",
            "score4_description": "The response consists of thoughtfully devised rewards for treasure discovery, obstacle avoidance, and oxygen management, but requires further optimization for balance or penalty precision.",
            "score5_description": "The response masterfully formulates a comprehensive reward function with balanced incentives for effective treasure discovery, strategic obstacle navigation, and prudent oxygen management, driving optimal explorer behavior."
        },
        "atomic_checklist": [
            "Does the response include a reward for moving closer to the underwater treasure location?",
            "Is there a notable penalty for colliding with obstacles to ensure careful navigation?",
            "Does the response foster oxygen conservation through rewards or penalties for wasteful consumption?",
            "Are the reward components correctly scaled or transformed to manage their impact on the total reward effectively?",
            "Does the response guarantee that the reward function complies with TorchScript, matching the environment's specifications?",
            "Are temperature parameters or scaling factors judiciously utilized to harmonize the influence of different reward components?",
            "Is the overall reward computation a coherent aggregation of individual components, effectively promoting the objective of efficient treasure discovery while safeguarding oxygen levels and avoiding obstacles?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "planning",
        "task": "reward_modeling",
        "instance_idx": 8,
        "system_prompt": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs. As an example,\nthe reward function signature can be: @torch.jit.script\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    ...\n    return reward, {}\n\nSince the reward function will be decorated with @torch.jit.script,\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward,\n    (2) a dictionary of each individual reward component.\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.",
        "input": "### User: The Python environment is class LavaFloorEscape(VecTask):\n    \"\"\"Rest of the environment definition omitted.\"\"\"\n    def compute_observations(self):\n\n        self.player_pos = torch.randn((1, 3), device=self.device) # Player position\n        self.exit_pos = torch.randn((1, 3), device=self.device) # Exit position\n        self.lava_positions = torch.randn((20, 3), device=self.device) # Lava positions\n        self.health_level = torch.tensor([1.0], device=self.device) # Full health\n\n        self.obs_buf = torch.cat((self.player_pos, self.exit_pos, self.lava_positions.flatten(), self.health_level), dim=-1)\n        return self.obs_buf\n. Write a reward function for the following task: to escape a room filled with lava by reaching the exit as quickly as possible without stepping on lava and maintaining health.",
        "reference_answer": "```python\nimport torch\nfrom torch import Tensor\nfrom typing import Tuple, Dict\n\n@torch.jit.script\ndef compute_reward(player_pos: Tensor, exit_pos: Tensor, lava_positions: Tensor, health_level: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n    # Constants for reward calculation\n    exit_temp: float = 0.1\n    health_temp: float = 0.05\n    lava_temp: float = 0.05\n\n    # Compute the distance to the exit\n    distance_to_exit = torch.norm(exit_pos - player_pos, dim=-1)\n\n    # Reward for health maintenance\n    health_reward = torch.exp(health_level / health_temp)\n\n    # Penalty for proximity to lava\n    min_dist_to_lava = torch.cdist(player_pos.unsqueeze(0), lava_positions).min()\n    lava_penalty = -torch.exp(-min_dist_to_lava / lava_temp)\n\n    # Construct individual rewards\n    exit_proximity_reward = -torch.exp(distance_to_exit / exit_temp)\n\n    # Total Reward\n    total_reward = exit_proximity_reward + health_reward + lava_penalty\n\n    # Reward components dictionary\n    reward_components: Dict[str, Tensor] = {\n        'exit_proximity_reward': exit_proximity_reward,\n        'health_reward': health_reward,\n        'lava_penalty': lava_penalty\n    }\n\n    return total_reward, reward_components\n\n# Placeholder tensors should be replaced with the actual environment observations\n# Example call to compute_reward function\n# reward, rewards_dict = compute_reward(dummy_player_pos, dummy_exit_pos, dummy_lava_positions, dummy_health_level)\n```",
        "score_rubric": {
            "criteria": "Does the response effectively design a reward function that guides the player to escape the lava-filled room quickly, avoid stepping on lava, and maintain health using relevant variables and reward transformations?",
            "score1_description": "The response fails to incentivize the player to reach the exit, avoid lava, or maintain health, missing the crucial components of the task.",
            "score2_description": "The response includes basic incentives for reaching the exit but neglects comprehensive strategies for lava avoidance or health maintenance, leading to an incomplete solution.",
            "score3_description": "The response offers rewards for reaching the exit and some means of lava avoidance or health conservation, yet lacks a cohesive strategy for integrating these factors together effectively.",
            "score4_description": "The response crafts well-thought-out rewards for escape, lava avoidance, and health maintenance, but could improve in optimization for balance among these components or precision in penalties.",
            "score5_description": "The response masterfully constructs a comprehensive reward function with balanced incentives for efficient escape, strategic lava navigation, and prudent health management, driving optimal player behavior."
        },
        "atomic_checklist": [
            "Does the response include a reward for moving closer to the exit position?",
            "Is there a notable penalty for stepping on lava to ensure careful navigation?",
            "Does the response encourage health maintenance through rewards or penalties for health loss?",
            "Are the reward components correctly scaled or transformed to manage their impact on the total reward effectively?",
            "Does the response guarantee that the reward function complies with TorchScript, matching the environment's specifications?",
            "Are temperature parameters or scaling factors judiciously utilized to harmonize the influence of different reward components?",
            "Is the overall reward computation a coherent aggregation of individual components, promoting the objective of efficiently escaping while avoiding lava and safeguarding health?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "planning",
        "task": "reward_modeling",
        "instance_idx": 9,
        "system_prompt": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs. As an example,\nthe reward function signature can be: @torch.jit.script\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    ...\n    return reward, {}\n\nSince the reward function will be decorated with @torch.jit.script,\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward,\n    (2) a dictionary of each individual reward component.\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.",
        "input": "### User: The Python environment is class UrbanParkCleanup(VecTask):\n    \"\"\"Rest of the environment definition omitted.\"\"\"\n    def compute_observations(self):\n\n        self.robot_pos = torch.randn((1, 3), device=self.device) # Robot position\n        self.trash_locations = torch.randn((15, 3), device=self.device) # Trash locations\n        battery_level = torch.tensor([0.8], device=self.device) # Battery level\n        self.obs_buf = torch.cat((self.robot_pos, self.trash_locations.flatten(), battery_level), dim=-1)\n        return self.obs_buf\n. Write a reward function for the following task: to clean up all the trash in the park efficiently while managing battery usage.",
        "reference_answer": "```python\nimport torch\nfrom torch import Tensor\nfrom typing import Tuple, Dict\n\n@torch.jit.script\ndef compute_reward(robot_pos: Tensor, trash_locations: Tensor, battery_level: Tensor, trash_collected: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n    # Constants for reward computation\n    trash_temp: float = 0.1\n    battery_temp: float = 0.05\n\n    # Compute the distance to the closest piece of trash\n    min_distance_to_trash = torch.cdist(robot_pos.unsqueeze(0), trash_locations).min()\n\n    # Calculate trash pickup reward\n    pickup_reward = -torch.exp(min_distance_to_trash / trash_temp) * trash_collected\n\n    # Reward for battery conservation\n    battery_reward = torch.exp(battery_level / battery_temp)\n\n    # Total Reward\n    total_reward = pickup_reward + battery_reward\n\n    # Reward components dictionary\n    reward_components: Dict[str, Tensor] = {\n        'pickup_reward': pickup_reward,\n        'battery_reward': battery_reward\n    }\n\n    return total_reward, reward_components\n\n# Placeholder tensors should be replaced with actual environment observations\n# Example call to compute_reward function\n# reward, rewards_dict = compute_reward(dummy_robot_pos, dummy_trash_locations, dummy_battery_level, dummy_trash_collected)\n```",
        "score_rubric": {
            "criteria": "Does the response effectively design a reward function that leads the cleaning robot to efficiently clean up the park, collecting all trash while conserving battery, using relevant variables and reward transformations?",
            "score1_description": "The response fails to incentivize trash collection, park coverage, or efficient battery usage, missing crucial components of the task.",
            "score2_description": "The response includes a basic reward for trash pickup but neglects comprehensive strategies for park coverage or battery conservation, leading to an incomplete solution.",
            "score3_description": "The response offers rewards for trash pickup and some form of battery conservation, but lacks coherent strategies for comprehensive park cleanup or the integration of these components.",
            "score4_description": "The response crafts well-thought-out rewards for trash collection, park coverage, and battery conservation, but could be optimized for balance among these components or clarity in penalties.",
            "score5_description": "The response masterfully formulates a comprehensive reward function with balanced incentives for complete trash collection, effective park coverage, and smart battery management, guiding optimal robot behavior."
        },
        "atomic_checklist": [
            "Does the response include a reward for each piece of trash collected by the robot?",
            "Is there a significant reward for completing the cleanup task with remaining battery power?",
            "Does the response introduce efficient battery usage strategies through rewards for minimal movement?",
            "Are the reward components properly scaled or transformed to manage their influence on the total reward?",
            "Does the response ensure the reward function is compatible with TorchScript, adhering to the environment's requirements?",
            "Are temperature parameters or scaling factors used thoughtfully to balance the contributions of different reward components?",
            "Is the total reward calculation coherent, effectively promoting efficient park cleanup while managing battery life?"
        ],
        "background_knowledge": []
    }
]
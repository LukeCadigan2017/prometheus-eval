[
    {
        "capability": "tool_usage",
        "task": "vision_tasks",
        "instance_idx": 0,
        "system_prompt": "You are a helpful assistant assisting the user's request.",
        "input": "Given the user request and the call command, the AI assistant helps the user to select a set of suitable model from a list of models according to the result from task planning. The AI assistant merely outputs the model ids of the most appropriate models. The output must be in a strict JSON format: [{\"id\": \"id\", \"reason\": \"your detail reason for the choice\"},...]. We have a list of models for you to choose from {'facebook/detr-resnet-50': {'task': 'object-detection'}, 'facebook/detr-resnet-101': {'task': 'object-detection'}, 'google/owlvit-base-patch32': {'task': 'object-detection'}, 'nlpconnect/vit-gpt2-image-captioning': {'task': 'image-to-text'}, 'ydshieh/vit-gpt2-coco-en': {'task': 'image-to-text'}, 'runwayml/stable-diffusion-v1-5': {'task': 'text-to-image'}, 'lambdalabs/sd-image-variations-diffusers': {'task': 'image-to-image'}, 'dandelin/vilt-b32-finetuned-vqa': {'task': 'visual-question-answering'}, 'impira/layoutlm-document-qa': {'task': 'document-question-answering'}, 'facebook/detr-resnet-50-panoptic': {'task': 'image-segmentation'}, 'facebook/maskformer-swin-base-coco': {'task': 'image-segmentation'}, 'Intel/dpt-large': {'task': 'depth-estimation'}, 'Intel/dpt-hybrid-midas': {'task': 'depth-estimation'}, 'damo-vilab/text-to-video-ms-1.7b': {'task': 'text-to-video'}, 'espnet/kan-bayashi_ljspeech_vits': {'task': 'text-to-speech'}, 'openai/whisper-base': {'task': 'automatic-speech-recognition'}, 'microsoft/speecht5_vc': {'task': 'audio-to-audio'}, 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k': {'task': 'audio-to-audio'}, 'google/vit-base-patch16-224': {'task': 'image-classification'}}. Please select a sequence of models from the list according to the task planning.\n\nQuestion: How much did the user spend for AMT per day in avergage?\nTask planning: 1. {'task':'document-question-answering, 'id':0, 'dep': [-1], 'args': {'image': 'tool_usage_vision_1.png}",
        "reference_answer": "To address the user's question regarding the average daily expenditure on AMT, the task planning outlines a single step that involves document-question-answering. Based on the provided list of models, the most suitable model for this task would be \"impira/layoutlm-document-qa\", which is specifically designed for document-question-answering tasks. Here is the model selection in strict JSON format:\n\n[\n   {\"id\": \"\\impira/layoutlm-document-qa\", \"reason\": \"This model is specifically designed for document-question-answering tasks, making it the most suitable choice for extracting and interpreting information from documents, such as determining average daily AMT expenditure from the provided image.\"}\n]",
        "score_rubric": {
            "criteria": "Does the response effectively utilize the provided tools to assist the user in selecting suitable models based on the task planning, and accurately output the model ids in the specified JSON format?",
            "score1_description": "The response fails to recognize the task planning requirements and selects models unrelated to the 'document-question-answering' task, resulting in an inaccurate and irrelevant model selection.",
            "score2_description": "The response identifies the 'document-question-answering' task but selects a model that is not the most appropriate for the task, indicating a misunderstanding of the models' capabilities.",
            "score3_description": "The response selects a relevant model for the 'document-question-answering' task but fails to provide the model ids in the correct JSON format, showing a lack of attention to the specified output format.",
            "score4_description": "The response selects an appropriate model for the 'document-question-answering' task and provides the model ids in JSON format, but with minor errors in formatting or completeness.",
            "score5_description": "The response accurately identifies the most appropriate model for the 'document-question-answering' task and outputs the model ids in the correct and complete JSON format, fully meeting the user's request."
        },
        "atomic_checklist": [
            "Does the response identify the 'document-question-answering' task as the primary task from the task planning?",
            "Does the response select the most suitable model ('impira/layoutlm-document-qa') for the 'document-question-answering' task?",
            "Is the selected model's id provided in the strict JSON output format as specified in the user request?",
            "Does the response include a detailed reason for the choice of the selected model in the JSON output?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "tool_usage",
        "task": "vision_tasks",
        "instance_idx": 1,
        "system_prompt": "You are a helpful assistant assisting the user's request.",
        "input": "Given the user request and the call command, the AI assistant helps the user to select a set of suitable model from a list of models according to the result from task planning. The AI assistant merely outputs the model ids of the most appropriate models. The output must be in a strict JSON format: [{\"id\": \"id\", \"reason\": \"your detail reason for the choice\"},...]. We have a list of models for you to choose from {'facebook/detr-resnet-50': {'task': 'object-detection'}, 'facebook/detr-resnet-101': {'task': 'object-detection'}, 'google/owlvit-base-patch32': {'task': 'object-detection'}, 'nlpconnect/vit-gpt2-image-captioning': {'task': 'image-to-text'}, 'ydshieh/vit-gpt2-coco-en': {'task': 'image-to-text'}, 'runwayml/stable-diffusion-v1-5': {'task': 'text-to-image'}, 'lambdalabs/sd-image-variations-diffusers': {'task': 'image-to-image'}, 'dandelin/vilt-b32-finetuned-vqa': {'task': 'visual-question-answering'}, 'impira/layoutlm-document-qa': {'task': 'document-question-answering'}, 'facebook/detr-resnet-50-panoptic': {'task': 'image-segmentation'}, 'facebook/maskformer-swin-base-coco': {'task': 'image-segmentation'}, 'Intel/dpt-large': {'task': 'depth-estimation'}, 'Intel/dpt-hybrid-midas': {'task': 'depth-estimation'}, 'damo-vilab/text-to-video-ms-1.7b': {'task': 'text-to-video'}, 'espnet/kan-bayashi_ljspeech_vits': {'task': 'text-to-speech'}, 'openai/whisper-base': {'task': 'automatic-speech-recognition'}, 'microsoft/speecht5_vc': {'task': 'audio-to-audio'}, 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k': {'task': 'audio-to-audio'}, 'google/vit-base-patch16-224': {'task': 'image-classification'}, 'openpose-control': {'task': 'pose-detection'}, 'lllyasviel/sd-controlnet-openpose': {'task': 'pose-text-to-image'}}. Please select a sequence of models from the list according to the task planning.\n\nQuestion: Transform the pose of character in tool_usage_vision_2_character.png to be same as the post of the person in tool_usage_vision_2_person.png. Task planning: 1: {'task': 'image-to-text', 'id':0, 'dep': [-1], 'args': {'image': 'tool_usage_vision_2_character.png'}\n2: {'task': 'pose-detection', 'id': 1, 'dep': [-1], 'args': {'image': 'tool_usage_vision_2_person.png'}\n3: {'task': 'pose-text-to-image', 'id':2, 'dep': [0,1], 'args': {'text': <resource>-0', 'image': '<resource>-1}}",
        "reference_answer": "To accomplish the given task of transforming the pose of a character in an image to match the pose of a person in another image, we need to follow the outlined task planning steps, which involve image-to-text, pose-detection, and then pose-text-to-image processes. Based on the provided list of models, we'll select the most appropriate models for each step:\n\n[\n    {\n        \"id\": \"ydshieh/vit-gpt2-coco-en\",\n        \"reason\": \"Chosen for its capability to generate detailed image captions, essential for accurate pose transformation.\"\n    },\n    {\n        \"id\": \"openpose-control\",\n        \"reason\": \"Specifically designed for pose detection, crucial for identifying the target pose.\"\n    },\n    {\n        \"id\": \"lllyasviel/sd-controlnet-openpose\",\n        \"reason\": \"Explicitly intended for generating images based on pose and text descriptions, perfect for the final transformation step.\"\n    }\n]",
        "score_rubric": {
            "criteria": "Does the response effectively utilize the provided list of models to select the most suitable ones for each step in the task planning, following the logical flow without directly solving the task?",
            "score1_description": "The response attempts to directly solve the task without utilizing the provided models, showing no understanding of the task planning.",
            "score2_description": "The response identifies some models but fails to match them correctly to the task planning steps, showing partial understanding but significant errors in tool usage.",
            "score3_description": "The response correctly identifies models for some of the task planning steps but misses or incorrectly assigns models for others, indicating moderate understanding and tool usage.",
            "score4_description": "The response correctly identifies suitable models for most task planning steps but makes minor errors in reasoning or model selection, indicating a good understanding of tool usage.",
            "score5_description": "The response correctly identifies the most suitable models for each task planning step and justifies the choices with logical reasoning, demonstrating excellent understanding and tool usage."
        },
        "atomic_checklist": [
            "Does the response correctly identify 'ydshieh/vit-gpt2-coco-en' for the image-to-text task?",
            "Is the model 'openpose-control' correctly selected for the pose-detection task?",
            "Does the response select 'lllyasviel/sd-controlnet-openpose' for the pose-text-to-image task, aligning with the final transformation step?",
            "Is the reasoning for each model selection clearly linked to its specific task in the task planning?",
            "Does the response format the output in strict JSON format as requested, without directly solving the task?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "tool_usage",
        "task": "vision_tasks",
        "instance_idx": 2,
        "system_prompt": "You are a helpful assistant assisting the user's request.",
        "input": "Given the user request and the call command, the AI assistant helps the user to select a set of suitable model from a list of models according to the result from task planning. The AI assistant merely outputs the model ids of the most appropriate models. The output must be in a strict JSON format: [{\"id\": \"id\", \"reason\": \"your detail reason for the choice\"},...]. We have a list of models for you to choose from {'facebook/detr-resnet-50': {'task': 'object-detection'}, 'facebook/detr-resnet-101': {'task': 'object-detection'}, 'google/owlvit-base-patch32': {'task': 'object-detection'}, 'nlpconnect/vit-gpt2-image-captioning': {'task': 'image-to-text'}, 'ydshieh/vit-gpt2-coco-en': {'task': 'image-to-text'}, 'runwayml/stable-diffusion-v1-5': {'task': 'text-to-image'}, 'lambdalabs/sd-image-variations-diffusers': {'task': 'image-to-image'}, 'dandelin/vilt-b32-finetuned-vqa': {'task': 'visual-question-answering'}, 'impira/layoutlm-document-qa': {'task': 'document-question-answering'}, 'facebook/detr-resnet-50-panoptic': {'task': 'image-segmentation'}, 'facebook/maskformer-swin-base-coco': {'task': 'image-segmentation'}, 'Intel/dpt-large': {'task': 'depth-estimation'}, 'Intel/dpt-hybrid-midas': {'task': 'depth-estimation'}, 'damo-vilab/text-to-video-ms-1.7b': {'task': 'text-to-video'}, 'espnet/kan-bayashi_ljspeech_vits': {'task': 'text-to-speech'}, 'openai/whisper-base': {'task': 'automatic-speech-recognition'}, 'microsoft/speecht5_vc': {'task': 'audio-to-audio'}, 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k': {'task': 'audio-to-audio'}, 'google/vit-base-patch16-224': {'task': 'image-classification'}}. Please select a sequence of models from the list according to the task planning.\nTask planning: 1: {\"task\": \"image-classification\", \"id\": 0, \"dep\": [-1], \"args\": {\"image\": \"tool_usage_vision_6.png\"}},\n2: {\"task\": \"visual-question-answering\", \"id\": 1, \"dep\": [-1], \"args\": {\"text\": \"What is the vehicle doing?\", \"image\": \"tool_usage_vision_6.png\"}},\n3: {\"task\": \"image-to-text\", \"id\": 2, \"dep\": [-1], \"args\": {\"image\": \"tool_usage_vision_6.png\"}}\"\n\nQuestion: In tool_usage_vision_3.png, identify the type of vehicle and its current activity.",
        "reference_answer": "To address the question of identifying the type of vehicle and its current activity in \"tool_usage_vision_3.png\", we need to select suitable models for each step of the task planning: image classification to identify the vehicle, visual question answering to understand its activity, and image-to-text to provide a detailed description. Based on the provided list of models, the selection would be as follows:\n\n[\n\t{\n\t\t\"id\": \"google/vit-base-patch16-224\",\n\t\t\"reason\": \"Selected for its robust performance in image classification tasks, essential for accurately identifying the type of vehicle.\"\n\t},\n\t{\n\t\t\"id\": \"dandelin/vilt-b32-finetuned-vqa\",\n\t\t\"reason\": \"Chosen for its specialization in visual question answering, crucial for understanding the vehicle's current activity.\"\n\t},\n\t{\n\t\t\"id\": \"ydshieh/vit-gpt2-coco-en\",\n\t\t\"reason\": \"Picked for its capability in generating detailed image captions, providing context and additional insights into the scene.\"\n\t}\n]",
        "score_rubric": {
            "criteria": "Does the response effectively utilize the tool's capabilities to select the most suitable models from the list based on the task planning, and is the output correctly formatted in strict JSON?",
            "score1_description": "The response incorrectly uses the tool's capabilities, selecting models that do not match the specified tasks in the task planning or fails to provide output in strict JSON format.",
            "score2_description": "The response makes a basic attempt to use the tool's capabilities, selecting at least one correct model for a task but either includes incorrect models for other tasks or provides output that only partially adheres to strict JSON format.",
            "score3_description": "The response demonstrates an adequate use of the tool's capabilities, selecting correct models for most tasks according to the task planning and providing output in strict JSON format, but with minor errors or omissions in model selection or reasoning.",
            "score4_description": "The response demonstrates a proficient use of the tool's capabilities, correctly selecting suitable models for all tasks according to the task planning and providing output in near-perfect strict JSON format, with only negligible errors.",
            "score5_description": "The response demonstrates an expert use of the tool's capabilities, perfectly selecting the most appropriate models for all tasks according to the task planning and providing output in flawless strict JSON format."
        },
        "atomic_checklist": [
            "Does the response correctly interpret the user's request and the task planning details?",
            "Does the response select models that are strictly relevant to the specified tasks in the task planning?",
            "Is the response's output correctly formatted in strict JSON, including both model ids and detailed reasons for each choice?",
            "Does the response provide a logical sequence of model selections that aligns with the flow of the task planning?",
            "Does the response include reasoning for each model selection that is clear and directly related to the task requirements?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "tool_usage",
        "task": "vision_tasks",
        "instance_idx": 3,
        "system_prompt": "You are a helpful assistant assisting the user's request.",
        "input": "Given the user request and the call command, the AI assistant helps the user to select a set of suitable model from a list of models according to the result from task planning. The AI assistant merely outputs the model ids of the most appropriate models. The output must be in a strict JSON format: [{model_id_for_task1}, {model_id_for_task2},...]. We have a list of models for you to choose from {'facebook/detr-resnet-50': {'task': 'object-detection'}, 'facebook/detr-resnet-101': {'task': 'object-detection'}, 'google/owlvit-base-patch32': {'task': 'object-detection'}, 'nlpconnect/vit-gpt2-image-captioning': {'task': 'image-to-text'}, 'ydshieh/vit-gpt2-coco-en': {'task': 'image-to-text'}, 'runwayml/stable-diffusion-v1-5': {'task': 'text-to-image'}, 'lambdalabs/sd-image-variations-diffusers': {'task': 'image-to-image'}, 'dandelin/vilt-b32-finetuned-vqa': {'task': 'visual-question-answering'}, 'impira/layoutlm-document-qa': {'task': 'document-question-answering'}, 'facebook/detr-resnet-50-panoptic': {'task': 'image-segmentation'}, 'facebook/maskformer-swin-base-coco': {'task': 'image-segmentation'}, 'Intel/dpt-large': {'task': 'depth-estimation'}, 'Intel/dpt-hybrid-midas': {'task': 'depth-estimation'}, 'damo-vilab/text-to-video-ms-1.7b': {'task': 'text-to-video'}, 'espnet/kan-bayashi_ljspeech_vits': {'task': 'text-to-speech'}, 'openai/whisper-base': {'task': 'automatic-speech-recognition'}, 'microsoft/speecht5_vc': {'task': 'audio-to-audio'}, 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k': {'task': 'audio-to-audio'}, 'google/vit-base-patch16-224': {'task': 'image-classification'}}. Please select a sequence of models from the list according to the task planning.\n\nQuestion: Replace the existing person in 'tool_usage_vision_4_place.png' with the person featured in 'tool_usage_vision_4_person.png'. This task involves identifying the person in 'tool_usage_vision_4_person.png', understanding the context of 'tool_usage_vision_4_place.png', and generating a new image that combines these elements.\n\nTask planning:\n\n1. {\"task\": \"image-to-text\",\"id\": 0,\"dep\": [-1],\"args\": {\"image\": \"tool_usage_vision_3_person.png\"}\n2. {\"task\": \"image-to-text\"\"id\": 1,\"dep\": [-1],\"args\": {\"image\": \"tool_usage_vision_4_png\"}\n3. {\"task\": \"text-to-image\",\"id\": 2,\"dep\": [0, 1],\"args\": {\"text\": \"Create an image placing the person described in <resource>-0 into the context of <resource>-1, matching the environment and pose as closely as possible.\"}}",
        "reference_answer": "For the given task of replacing a person in one image with another from a different image, while understanding the context and generating a new image that combines these elements, the selection of models based on the task planning would be as follows:\n\n[\n\t{\n\t\t\"id\": \"ydshieh/vit-gpt2-coco-en\",\n\t\t\"reason\": \"Chosen for its capability to generate detailed descriptions of the person in 'tool_usage_vision_4_person.png', essential for accurate replacement.\"\n\t},\n\t{\n\t\t\"id\": \"ydshieh/vit-gpt2-coco-en\",\n\t\t\"reason\": \"Selected for its ability to understand the context of 'tool_usage_vision_4_place.png' by providing a detailed scene description.\"\n\t},\n\t{\n\t\t\"id\": \"runwayml/stable-diffusion-v1-5\",\n\t\t\"reason\": \"Utilized for its text-to-image conversion capability, enabling the creation of a new image by combining the person and place descriptions.\"\n\t}\n]",
        "score_rubric": {
            "criteria": "Does the response correctly identify and select the most appropriate models for each step of the task planning, matching the tasks with the capabilities of the models from the provided list?",
            "score1_description": "The response selects incorrect models for all tasks, showing a lack of understanding of the models' capabilities and the task requirements.",
            "score2_description": "The response selects partially correct models for the tasks but makes significant errors in matching the models' capabilities to the task requirements, such as selecting a text-to-speech model for an image-to-text task.",
            "score3_description": "The response selects mostly correct models but makes a minor error in one of the selections, such as choosing a model that is slightly less suited for the task's specifics, like selecting 'facebook/detr-resnet-50' instead of 'facebook/detr-resnet-101' for a more detailed object-detection requirement.",
            "score4_description": "The response selects the correct models for each task but provides insufficient or generic reasoning for the choices, not clearly demonstrating an understanding of why each model is the most suited for its respective task.",
            "score5_description": "The response accurately selects the most appropriate models for each task and provides clear, specific reasoning for each choice, demonstrating a deep understanding of the models' capabilities and how they match the task requirements."
        },
        "atomic_checklist": [
            "Does the response select 'ydshieh/vit-gpt2-coco-en' for both the first and second tasks, recognizing its suitability for generating detailed descriptions of people and scenes?",
            "Does the response choose 'runwayml/stable-diffusion-v1-5' for the third task, recognizing its capability for text-to-image conversion to combine person and place descriptions?",
            "Is the response's reasoning for each model selection specific and tied to the tasks' requirements, such as detailing why 'ydshieh/vit-gpt2-coco-en' is suited for image-to-text conversion based on the need for detailed descriptions?",
            "Does the response ensure the output is in strict JSON format, reflecting the structure and requirements specified in the user request?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "tool_usage",
        "task": "vision_tasks",
        "instance_idx": 4,
        "system_prompt": "You are a helpful assistant assisting the user's request.",
        "input": "Given the user request and the call command, the AI assistant helps the user to select a set of suitable model from a list of models according to the result from task planning. The AI assistant merely outputs the model ids of the most appropriate models. The output must be in a strict JSON format: [{\"id\": \"id\", \"reason\": \"your detail reason for the choice\"},...]. We have a list of models for you to choose from {'facebook/detr-resnet-50': {'task': 'object-detection'}, 'facebook/detr-resnet-101': {'task': 'object-detection'}, 'google/owlvit-base-patch32': {'task': 'object-detection'}, 'nlpconnect/vit-gpt2-image-captioning': {'task': 'image-to-text'}, 'ydshieh/vit-gpt2-coco-en': {'task': 'image-to-text'}, 'runwayml/stable-diffusion-v1-5': {'task': 'text-to-image'}, 'lambdalabs/sd-image-variations-diffusers': {'task': 'image-to-image'}, 'dandelin/vilt-b32-finetuned-vqa': {'task': 'visual-question-answering'}, 'impira/layoutlm-document-qa': {'task': 'document-question-answering'}, 'facebook/detr-resnet-50-panoptic': {'task': 'image-segmentation'}, 'facebook/maskformer-swin-base-coco': {'task': 'image-segmentation'}, 'Intel/dpt-large': {'task': 'depth-estimation'}, 'Intel/dpt-hybrid-midas': {'task': 'depth-estimation'}, 'damo-vilab/text-to-video-ms-1.7b': {'task': 'text-to-video'}, 'espnet/kan-bayashi_ljspeech_vits': {'task': 'text-to-speech'}, 'openai/whisper-base': {'task': 'automatic-speech-recognition'}, 'microsoft/speecht5_vc': {'task': 'audio-to-audio'}, 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k': {'task': 'audio-to-audio'}, 'google/vit-base-patch16-224': {'task': 'image-classification'}}. Please select a sequence of models from the list according to the task planning.\n\nQuestion: In tool_usage_vision_5.png, what's the animal and what does it looking at?\nTask Planning:\n1: {\"task\": \"image-classification\", \"id\": 0, dep: [-1], \"args\": {\"image\": \"tool_usage_vision_5.png\" }},\n2: {\"task\": \"visual-question-answering\", \"id\": 1, dep: [-1], \"args\": {\"text\": \"What is the animal doing?\",\n\"image\": \"tool_usage_vision_5.png\" }}\n3: {\"task\":\"image-to-text\", \"id\": 2, dep: [-1], \"args\": {\"image\": \"tool_usage_vision_5.png\" }}",
        "reference_answer": "For the task of identifying the animal in \"tool_usage_vision_5.png\" and understanding what it is looking at, the appropriate models to select from the provided list, according to the task planning steps, would be as follows:\n\n[\n\t{\n\t\t\"id\": \"google/vit-base-patch16-224\",\n\t\t\"reason\": \"Selected for its image-classification capability to identify the animal in 'tool_usage_vision_5.png'.\"\n\t},\n\t{\n\t\t\"id\": \"dandelin/vilt-b32-finetuned-vqa\",\n\t\t\"reason\": \"Chosen for its ability to answer visual questions, aiding in understanding what the animal is doing.\"\n\t},\n\t{\n\t\t\"id\": \"ydshieh/vit-gpt2-coco-en\",\n\t\t\"reason\": \"Utilized for generating a detailed description of the scene, potentially revealing more about the animal's focus.\"\n\t}\n]",
        "score_rubric": {
            "criteria": "Does the response accurately match the task requirements with the capabilities of the specific models from the provided list?",
            "score1_description": "The response incorrectly selects models such as 'espnet/kan-bayashi_ljspeech_vits' for text-to-speech instead of 'google/vit-base-patch16-224' for image-classification, showing a fundamental misunderstanding of the tasks and model capabilities.",
            "score2_description": "The response selects a partially correct model like 'facebook/detr-resnet-50' for object detection instead of 'google/vit-base-patch16-224' for image-classification, indicating a confusion in task specialization.",
            "score3_description": "The response correctly identifies 'google/vit-base-patch16-224' for image-classification but chooses 'impira/layoutlm-document-qa' instead of 'dandelin/vilt-b32-finetuned-vqa' for visual-question-answering, showing minor errors in task-model alignment.",
            "score4_description": "The response selects the correct models 'google/vit-base-patch16-224' for image-classification and 'dandelin/vilt-b32-finetuned-vqa' for visual-question-answering but provides generic reasoning, lacking depth in the explanation of model capabilities.",
            "score5_description": "The response accurately selects 'google/vit-base-patch16-224' for image-classification, 'dandelin/vilt-b32-finetuned-vqa' for visual-question-answering, and 'ydshieh/vit-gpt2-coco-en' for image-to-text, with detailed reasoning that demonstrates a deep understanding of each model's specific capabilities in relation to the tasks."
        },
        "atomic_checklist": [
            "Does the response accurately select 'google/vit-base-patch16-224' for the task of image-classification to identify the animal?",
            "Does the response appropriately choose 'dandelin/vilt-b32-finetuned-vqa' for the visual-question-answering task to discern what the animal is doing?",
            "Does the response correctly utilize 'ydshieh/vit-gpt2-coco-en' for the image-to-text task for generating a detailed scene description?",
            "Is the response's reasoning for each model selection detailed and specific, effectively explaining the match between the model's capabilities and the task requirements?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "tool_usage",
        "task": "vision_tasks",
        "instance_idx": 5,
        "system_prompt": "You are a helpful assistant assisting the user's request.",
        "input": "Given the user request and the call command, the AI assistant helps the user to select a set of suitable models from a list of models according to the result from task planning. The AI assistant merely outputs the model ids of the most appropriate models. The output must be in a strict JSON format: [{\"id\": \"id\", \"reason\": \"your detail reason for the choice\"},...]. We have a list of models for you to choose from {'facebook/detr-resnet-50': {'task': 'object-detection'}, 'facebook/detr-resnet-101': {'task': 'object-detection'}, 'google/owlvit-base-patch32': {'task': 'object-detection'}, 'nlpconnect/vit-gpt2-image-captioning': {'task': 'image-to-text'}, 'ydshieh/vit-gpt2-coco-en': {'task': 'image-to-text'}, 'runwayml/stable-diffusion-v1-5': {'task': 'text-to-image'}, 'lambdalabs/sd-image-variations-diffusers': {'task': 'image-to-image'}, 'dandelin/vilt-b32-finetuned-vqa': {'task': 'visual-question-answering'}, 'impira/layoutlm-document-qa': {'task': 'document-question-answering'}, 'facebook/detr-resnet-50-panoptic': {'task': 'image-segmentation'}, 'facebook/maskformer-swin-base-coco': {'task': 'image-segmentation'}, 'Intel/dpt-large': {'task': 'depth-estimation'}, 'Intel/dpt-hybrid-midas': {'task': 'depth-estimation'}, 'damo-vilab/text-to-video-ms-1.7b': {'task': 'text-to-video'}, 'espnet/kan-bayashi_ljspeech_vits': {'task': 'text-to-speech'}, 'openai/whisper-base': {'task': 'automatic-speech-recognition'}, 'microsoft/speecht5_vc': {'task': 'audio-to-audio'}, 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k': {'task': 'audio-to-audio'}, 'google/vit-base-patch16-224': {'task': 'image-classification'}}. Please select a sequence of models from the list according to the task planning.\n\nQuestion: Synthesize a background monologue for a historical documentary using voice clips found in 'tool_usage_vision_8_audio_clips.png'.\nTask Planning: 1: {'task': 'document-question-answering', 'id':0, 'dep': [-1], 'args': {'image': 'tool_usage_vision_8_audio_clips.png'}},\n2: {'task': 'text-to-speech', 'id': 1, 'dep': [0], 'args': {'text': '<resource>-0'}}",
        "reference_answer": "To synthesize a background monologue for a historical documentary using voice clips described in 'tool_usage_vision_8_audio_clips.png', we follow the task planning steps of extracting detailed information from the document and then converting this text into speech. The selected models according to this task planning are:\n\n[\n\t{\n\t\t\"id\": \"impira/layoutlm-document-qa\",\n\t\t\"reason\": \"Selected for its unparalleled capabilities in document-question-answering, essential for accurately interpreting information from the document containing voice clips descriptions.\"\n\t},\n\t{\n\t\t\"id\": \"espnet/kan-bayashi_ljspeech_vits\",\n\t\t\"reason\": \"Chosen for its superior text-to-speech conversion, which will ensure a natural and engaging monologue for the documentary's background narration.\"\n\t}\n]",
        "score_rubric": {
            "criteria": "Does the response effectively select the most suitable models from the list based on the task planning, accurately identify the tasks, and provide the output in the required JSON format?",
            "score1_description": "The response fails to select models that match the tasks specified in the task planning or does not adhere to the strict JSON output format.",
            "score2_description": "The response selects at least one correct model but fails to accurately match another model to the task planning steps or slightly deviates from the strict JSON format.",
            "score3_description": "The response correctly selects models for each task but either chooses models that are not the best fit given the specifics of the task or has minor errors in the JSON formatting.",
            "score4_description": "The response effectively selects the most suitable models for each task and formats the output in strict JSON format, with only negligible errors in reasoning or formatting.",
            "score5_description": "The response perfectly selects the most appropriate models for each task according to the task planning, provides well-justified reasoning for each choice, and formats the output in flawless strict JSON."
        },
        "atomic_checklist": [
            "Does the response correctly identify 'impira/layoutlm-document-qa' for the document-question-answering task to interpret audio clip information?",
            "Is the model 'espnet/kan-bayashi_ljspeech_vits' correctly selected for the text-to-speech task to synthesize a background monologue?",
            "Is the reasoning for model selection detailed and directly linked to the task requirements?",
            "Does the composition adhere to the strict JSON format with clear identification and reasoning for each model chosen?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "tool_usage",
        "task": "vision_tasks",
        "instance_idx": 6,
        "system_prompt": "You are a helpful assistant assisting the user's request.",
        "input": "Given the user request and the call command, the AI assistant helps the user to select a set of suitable model from a list of models according to the result from task planning. The AI assistant merely outputs the model ids of the most appropriate models. The output must be in a strict JSON format: [{\"id\": \"id\", \"reason\": \"your detail reason for the choice\"},...]. We have a list of models for you to choose from {'facebook/detr-resnet-50': {'task': 'object-detection'}, 'facebook/detr-resnet-101': {'task': 'object-detection'}, 'google/owlvit-base-patch32': {'task': 'object-detection'}, 'nlpconnect/vit-gpt2-image-captioning': {'task': 'image-to-text'}, 'ydshieh/vit-gpt2-coco-en': {'task': 'image-to-text'}, 'runwayml/stable-diffusion-v1-5': {'task': 'text-to-image'}, 'lambdalabs/sd-image-variations-diffusers': {'task': 'image-to-image'}, 'dandelin/vilt-b32-finetuned-vqa': {'task': 'visual-question-answering'}, 'impira/layoutlm-document-qa': {'task': 'document-question-answering'}, 'facebook/detr-resnet-50-panoptic': {'task': 'image-segmentation'}, 'facebook/maskformer-swin-base-coco': {'task': 'image-segmentation'}, 'Intel/dpt-large': {'task': 'depth-estimation'}, 'Intel/dpt-hybrid-midas': {'task': 'depth-estimation'}, 'damo-vilab/text-to-video-ms-1.7b': {'task': 'text-to-video'}, 'espnet/kan-bayashi_ljspeech_vits': {'task': 'text-to-speech'}, 'openai/whisper-base': {'task': 'automatic-speech-recognition'}, 'microsoft/speecht5_vc': {'task': 'audio-to-audio'}, 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k': {'task': 'audio-to-audio'}, 'google/vit-base-patch16-224': {'task': 'image-classification'}}. Please select a sequence of models from the list according to the task planning.\n\nQuestion: Extract key healthcare advice from 'tool_usage_vision_9_health_documents.png' and convert this information into an interactive Q&A audio session.\nTask Planning: 1: {'task': 'document-question-answering', 'id':0, 'dep': [-1], 'args': {'image': 'tool_usage_vision_9_health_documents.png'}},\n2: {'task': 'text-to-speech', 'id': 1, 'dep': [0], 'args': {'text': '<resource>-0'}},\n3: {'task': 'audio-to-audio', 'id': 2, 'dep': [1], 'args': {'audio': 'question_prompts', 'interactivity': 'yes'}}",
        "reference_answer": "To extract key healthcare advice from 'tool_usage_vision_9_health_documents.png' and convert this information into an interactive Q&A audio session, the process requires special attention to correctly interpret the document, synthesize the extracted text into speech, and then further process the audio to add interactive capabilities. The chosen models are detailed as follows:\n\n[\n\t{\n\t\t\"id\": \"impira/layoutlm-document-qa\",\n\t\t\"reason\": \"Selected for its robust capabilities in document-question-answering, vital for extracting precise healthcare advice from complex documents.\"\n\t},\n\t{\n\t\t\"id\": \"espnet/kan-bayashi_ljspeech_vits\",\n\t\t\"reason\": \"Chosen for its exceptional text-to-speech conversion, essential for creating clear and engaging audio from the extracted text.\"\n\t},\n\t{\n\t\t\"id\": \"microsoft/speecht5_vc\",\n\t\t\"reason\": \"Utilized for its advanced audio-to-audio processing, necessary for embedding interactivity into the session, allowing users to engage with the advice through Q&A.\"\n\t}\n]",
        "score_rubric": {
            "criteria": "Does the response accurately utilize the provided models to select the most suitable ones for each step of the task planning, clearly interpreting the tasks, and formatting the output within the specified JSON parameters?",
            "score1_description": "The response shows a failure to accurately identify any suitable models for the tasks, along with a lack of adherence to the required JSON output format.",
            "score2_description": "The response shows partial accuracy in selecting a model for one of the tasks but fails or displays significant inaccuracies in matching other tasks to models, along with potential deviations from the JSON format.",
            "score3_description": "The response accurately selects models for the majority of tasks with logically sound justification but includes minor inaccuracy in model selection for one task or slight errors in JSON formatting.",
            "score4_description": "The response accurately selects models perfectly matching the tasks with well-justified reasoning and presents it in a nearly perfect JSON format, with minor issues in the detailed justification.",
            "score5_description": "The response exhibits expert model selection that perfectly fits the tasks, provides detailed explanations for each choice, and ensures flawless JSON formatting."
        },
        "atomic_checklist": [
            "Does the response correctly choose 'impira/layoutlm-document-qa' for extracting healthcare advice from the document?",
            "Is 'espnet/kan-bayashi_ljspeech_vits' accurately selected for converting the extracted advice into speech?",
            "Does the response select 'microsoft/speecht5_vc' for enhancing the audio with Q&A interactive functionality?",
            "Is each model's choice clearly reasoned in terms of its specific task utility?",
            "Does the output accurately follow the specified strict JSON format?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "tool_usage",
        "task": "vision_tasks",
        "instance_idx": 7,
        "system_prompt": "You are a helpful assistant assisting the user's request.",
        "input": "Given the user request and the call command, the AI assistant helps the user to select a set of suitable models from a list of models according to the result from task planning. The AI assistant merely outputs the model ids of the most appropriate models. The output must be in a strict JSON format: [{\"id\": \"id\", \"reason\": \"your detail reason for the choice\"},...]. We have a list of models for you to choose from {'facebook/detr-resnet-50': {'task': 'object-detection'}, 'facebook/detr-resnet-101': {'task': 'object-detection'}, 'google/owlvit-base-patch32': {'task': 'object-detection'}, 'nlpconnect/vit-gpt2-image-captioning': {'task': 'image-to-text'}, 'ydshieh/vit-gpt2-coco-en': {'task': 'image-to-text'}, 'runwayml/stable-diffusion-v1-5': {'task': 'text-to-image'}, 'lambdalabs/sd-image-variations-diffusers': {'task': 'image-to-image'}, 'dandelin/vilt-b32-finetuned-vqa': {'task': 'visual-question-answering'}, 'impira/layoutlm-document-qa': {'task': 'document-question-answering'}, 'facebook/detr-resnet-50-panoptic': {'task': 'image-segmentation'}, 'facebook/maskformer-swin-base-coco': {'task': 'image-segmentation'}, 'Intel/dpt-large': {'task': 'depth-estimation'}, 'Intel/dpt-hybrid-midas': {'task': 'depth-estimation'}, 'damo-vilab/text-to-video-ms-1.7b': {'task': 'text-to-video'}, 'espnet/kan-bayashi_ljspeech_vits': {'task': 'text-to-speech'}, 'openai/whisper-base': {'task': 'automatic-speech-recognition'}, 'microsoft/speecht5_vc': {'task': 'audio-to-audio'}, 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k': {'task': 'audio-to-audio'}, 'google/vit-base-patch16-224': {'task': 'image-classification'}}. Please select a sequence of models from the list according to the task planning.\n\nQuestion: Design an interactive educational app feature that generates custom quizzes based on the content of 'tool_usage_vision_10_educational_material.png'.\nTask Planning: 1: {'task': 'image-to-text', 'id':0, 'dep': [-1], 'args': {'image': 'tool_usage_vision_10_educational_material.png'}},\n2: {'task': 'text-to-video', 'id': 1, 'dep': [0], 'args': {'text': '<resource>-0', 'quiz_format': 'multiple-choice'}}",
        "reference_answer": "To design an interactive educational app feature that generates custom quizzes based on the content of 'tool_usage_vision_10_educational_material.png', the first step involves converting the image to text, followed by the formation of multiple-choice quizzes in a video format. The selected models for these steps are detailed below:\n\n[\n\t{\n\t\t\"id\": \"ydshieh/vit-gpt2-coco-en\",\n\t\t\"reason\": \"Chosen for its proficiency in image-to-text conversion, essential for analyzing and extracting quiz content from educational materials.\"\n\t},\n\t{\n\t\t\"id\": \"damo-vilab/text-to-video-ms-1.7b\",\n\t\t\"reason\": \"Selected for its ability to transform text into interactive video quizzes, enhancing learning engagement through visual aids.\"\n\t}\n]",
        "score_rubric": {
            "criteria": "Does the response accurately select models based on their ability to perform specific tasks outlined in the task planning, including understanding content and quiz generation, and correctly format the output in JSON?",
            "score1_description": "The response fails to select appropriate models for understanding educational content or quiz generation, and does not follow strict JSON formatting.",
            "score2_description": "The response selects an appropriate model for either understanding content or quiz generation but not both, or struggles with JSON formatting.",
            "score3_description": "The response selects suitable models for both tasks but may not choose the optimal models available, with minor JSON format errors.",
            "score4_description": "The response effectively selects appropriate models for understanding educational content and quiz generation, with nearly correct JSON formatting.",
            "score5_description": "The response expertly selects the best models for both understanding educational content and generating quizzes, with flawless JSON output formatting."
        },
        "atomic_checklist": [
            "Does the response correctly identify 'ydshieh/vit-gpt2-coco-en' for converting the document image to editable text?",
            "Is the model 'damo-vilab/text-to-video-ms-1.7b' accurately selected for creating custom quizzes in a video format from the text?",
            "Does the explanation for model selection clearly relate to their specific capabilities?",
            "Is the output provided in strict JSON format with detailed reasons for each choice?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "tool_usage",
        "task": "vision_tasks",
        "instance_idx": 8,
        "system_prompt": "You are a helpful assistant assisting the user's request.",
        "input": "Given the user request and the call command, the AI assistant helps the user to select a set of suitable models from a list of models according to the result from task planning. The AI assistant merely outputs the model ids of the most appropriate models. The output must be in a strict JSON format: [{\"id\": \"id\", \"reason\": \"your detail reason for the choice\"},...]. We have a list of models for you to choose from {'facebook/detr-resnet-50': {'task': 'object-detection'}, 'facebook/detr-resnet-101': {'task': 'object-detection'}, 'google/owlvit-base-patch32': {'task': 'object-detection'}, 'nlpconnect/vit-gpt2-image-captioning': {'task': 'image-to-text'}, 'ydshieh/vit-gpt2-coco-en': {'task': 'image-to-text'}, 'runwayml/stable-diffusion-v1-5': {'task': 'text-to-image'}, 'lambdalabs/sd-image-variations-diffusers': {'task': 'image-to-image'}, 'dandelin/vilt-b32-finetuned-vqa': {'task': 'visual-question-answering'}, 'impira/layoutlm-document-qa': {'task': 'document-question-answering'}, 'facebook/detr-resnet-50-panoptic': {'task': 'image-segmentation'}, 'facebook/maskformer-swin-base-coco': {'task': 'image-segmentation'}, 'Intel/dpt-large': {'task': 'depth-estimation'}, 'Intel/dpt-hybrid-midas': {'task': 'depth-estimation'}, 'damo-vilab/text-to-video-ms-1.7b': {'task': 'text-to-video'}, 'espnet/kan-bayashi_ljspeech_vits': {'task': 'text-to-speech'}, 'openai/whisper-base': {'task': 'automatic-speech-recognition'}, 'microsoft/speecht5_vc': {'task': 'audio-to-audio'}, 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k': {'task': 'audio-to-audio'}, 'google/vit-base-patch16-224': {'task': 'image-classification'}}. Please select a sequence of models from the list according to the task planning.\n\nQuestion: Develop a feature for real-time translation of street signs from 'tool_usage_vision_11_street_signs.png' into multiple languages for a navigation app.\nTask Planning: 1: {'task': 'image-to-text', 'id':0, 'dep': [-1], 'args': {'image': 'tool_usage_vision_11_street_signs.png'}},\n2: {'task': 'automatic-speech-recognition', 'id': 1, 'dep': [0], 'args': {'text': '<resource>-0'}},\n3: {'task': 'text-to-speech', 'id': 2, 'dep': [1], 'args': {'text': '<resource>-1', 'languages': ['Spanish', 'French', 'German']}}",
        "reference_answer": "To develop a real-time translation feature for street signs in a navigation app, addressing the image 'tool_usage_vision_11_street_signs.png', the procedure involves steps for image-to-text conversion, speech recognition of the converted text, and text-to-speech output in several languages. Appropriate models aligning with this task planning are:\n\n[\n\t{\n\t\t\"id\": \"ydshieh/vit-gpt2-coco-en\",\n\t\t\"reason\": \"Optimal for its ability to accurately transcribe the varied textual content of street signs into editable text, setting the foundation for translation.\"\n\t},\n\t{\n\t\t\"id\": \"openai/whisper-base\",\n\t\t\"reason\": \"Chosen for its advanced speech recognition capability necessary to correctly interpret the textual translations for accurate voice output.\"\n\t},\n\t{\n\t\t\"id\": \"espnet/kan-bayashi_ljspeech_vits\",\n\t\t\"reason\": \"Ideal for its multilingual text-to-speech synthesis, enabling the conversion of text translations into audible speech in Spanish, French, and German for the app user.\"\n\t}\n]",
        "score_rubric": {
            "criteria": "Does the response expertly choose models based on their capability to perform the essential tasks in the task planning for real-time translation of street signs, with the output formatted in JSON?",
            "score1_description": "The response inaccurately matches models to the outlined tasks, failing in both task understanding and JSON formatting.",
            "score2_description": "The response accurately selects a model for at least one task but inaccurately chooses or justifies selections for other tasks, with minor JSON formatting errors.",
            "score3_description": "The response suitably selects models for the tasks but may not choose the optimal model for task specificity or has minor issues with JSON format.",
            "score4_description": "The response convincingly selects appropriate models for the tasks with sound reasoning behind each choice and mostly adheres to the JSON format, containing very minor errors.",
            "score5_description": "The response exemplifies perfection in selecting the most suited models for all tasks with comprehensive reasoning for each choice, formatted flawlessly in strict JSON."
        },
        "atomic_checklist": [
            "Does the response accurately choose 'ydshieh/vit-gpt2-coco-en' for converting street sign images to text?",
            "Is 'openai/whisper-base' effectively selected for recognizing speech patterns in the translated text useful for accurate speech recognition?",
            "Does the model 'espnet/kan-bayashi_ljspeech_vits' get selected for converting translated text into speech in various languages?",
            "Is the detailed explanation for each model selection closely tied to its role in the real-time translation feature?",
            "Is every part of the response meticulously formatted in compliant JSON structure with clear reasons for model selections?"
        ],
        "background_knowledge": []
    },
    {
        "capability": "tool_usage",
        "task": "vision_tasks",
        "instance_idx": 9,
        "system_prompt": "You are a helpful assistant assisting the user's request.",
        "input": "Given the user request and the call command, the AI assistant helps the user to select a set of suitable models from a list of models according to the result from task planning. The AI assistant merely outputs the model ids of the most appropriate models. The output must be in a strict JSON format: [{\"id\": \"id\", \"reason\": \"your detail reason for the choice\"},...]. We have a list of models for you to choose from {'facebook/detr-resnet-50': {'task': 'object-detection'}, 'facebook/detr-resnet-101': {'task': 'object-detection'}, 'google/owlvit-base-patch32': {'task': 'object-detection'}, 'nlpconnect/vit-gpt2-image-captioning': {'task': 'image-to-text'}, 'ydshieh/vit-gpt2-coco-en': {'task': 'image-to-text'}, 'runwayml/stable-diffusion-v1-5': {'task': 'text-to-image'}, 'lambdalabs/sd-image-variations-diffusers': {'task': 'image-to-image'}, 'dandelin/vilt-b32-finetuned-vqa': {'task': 'visual-question-answering'}, 'impira/layoutlm-document-qa': {'task': 'document-question-answering'}, 'facebook/detr-resnet-50-panoptic': {'task': 'image-segmentation'}, 'facebook/maskformer-swin-base-coco': {'task': 'image-segmentation'}, 'Intel/dpt-large': {'task': 'depth-estimation'}, 'Intel/dpt-hybrid-midas': {'task': 'depth-estimation'}, 'damo-vilab/text-to-video-ms-1.7b': {'task': 'text-to-video'}, 'espnet/kan-bayashi_ljspeech_vits': {'task': 'text-to-speech'}, 'openai/whisper-base': {'task': 'automatic-speech-recognition'}, 'microsoft/speecht5_vc': {'task': 'audio-to-audio'}, 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k': {'task': 'audio-to-audio'}, 'google/vit-base-patch16-224': {'task': 'image-classification'}}. Please select a sequence of models from the list according to the task planning.\n\nQuestion: Produce a guided meditation audio track that integrates visual imagery description from 'tool_usage_vision_12_meditation_scenes.png'.\nTask Planning: 1: {'task': 'image-to-text', 'id':0, 'dep': [-1], 'args': {'image': 'tool_usage_vision_12_meditation_scenes.png'}},\n2: {'task': 'text-to-speech', 'id': 1, 'dep': [0], 'args': {'text': '<resource>-0', 'tone': 'calm'}},\n3: {'task': 'audio-to-audio', 'id': 2, 'dep': [1], 'args': {'audio': 'ambient_music', 'overlay': 'yes'}}",
        "reference_answer": "To produce a guided meditation audio track that integrates visual imagery description from 'tool_usage_vision_12_meditation_scenes.png', the task involves transforming imagery into descriptive text, synthesizing this description into a soothing narration, and overlaying it with ambient music. Hence, the sequence of models selected are:\n\n[\n\t{\n\t\t\"id\": \"ydshieh/vit-gpt2-coco-en\",\n\t\t\"reason\": \"Selected for its capability to accurately convert images into descriptive text, providing a foundation for the calm narration.\"\n\t},\n\t{\n\t\t\"id\": \"espnet/kan-bayashi_ljspeech_vits\",\n\t\t\"reason\": \"Chosen for its proficiency in generating speech from text with the capability to adjust tonality to the required calmness for meditation.\"\n\t},\n\t{\n\t\t\"id\": \"JorisCos/DCCRNet_Libri1Mix_enhsingle_16k\",\n\t\t\"reason\": \"Utilized for overlaying the calm narration with ambient music, enhancing the meditative quality of the audio track.\"\n\t}\n]",
        "score_rubric": {
            "criteria": "Does the response select models based on their ability to accurately perform the tasks required for creating the guided meditation audio track, including analyzing visual imagery, synthesizing descriptive narration, and incorporating ambient music, and output provided in JSON format?",
            "score1_description": "The response incorrectly selects unsuitable models for the tasks, fails to justify the choices logically, and does not adhere to JSON formatting requirements.",
            "score2_description": "The response shows a partial understanding by selecting a correct model for one task but selects inadequately for others and has minor deviations from JSON format.",
            "score3_description": "The response selects appropriate models for most tasks with rational justifications but may choose suboptimal models for one task or has minor JSON formatting inaccuracies.",
            "score4_description": "The response demonstrates a good selection of models for all tasks with detailed justifications and almost perfect adherence to the JSON format, with very minor errors.",
            "score5_description": "The response expertly identifies the best models for each task with precise and convincing justifications, formatting the output flawlessly in JSON."
        },
        "atomic_checklist": [
            "Does the response correctly identify 'ydshieh/vit-gpt2-coco-en' for converting meditation scene images to a descriptive text?",
            "Is 'espnet/kan-bayashi_ljspeech_vits' accurately chosen for creating a calm-toned speech from the descriptive text?",
            "Does the response select 'JorisCos/DCCRNet_Libri1Mix_enhsingle_16k' for integrating ambient music with the calm-toned speech?",
            "Is the reason behind each model selection tightly connected to its contribution towards creating the meditation audio track?",
            "Does the model information and explanations for selection strictly follow the JSON format as specified?"
        ],
        "background_knowledge": []
    }
]